\documentclass[11pt, oneside, a4paper]{article}
\input{preamble.tex}
%---header/style/enumeration-------
\pagestyle{fancy}
\lhead{Capstone}
\chead{MCS}
\rhead{2018-2019 Semester 1}
\author{Linfan}
%----------------------------------

%-----------MY INFORMATION---------
%\title{{\fontfamily{qbk}\selectfont
\title{Reading notes}
\date{\vspace{-5ex}}
%----------------------------------
%unnumbered sections in TOC
%\setcounter{secnumdepth}{0}

%-----This is where the GOOD STUFF begins---
\begin{document}

\maketitle

\thispagestyle{fancy}

This document contains all notes taken while reading materials (e.g., textbooks, literature) in preparation for the capstone project. Black text are information consolidated from the readings; blue text are notes (proofs, explanations); red text are questions.

\tableofcontents
\listoftodos

% \DAS{This is great progress, Linda. Is there a theorem in here that allows us to explicitly construct (a valid example of the nonunique) $U^+$ from $U$?}
% {\todo[inline]{Look at Theorem \ref{thm:condition iff adjoint}}}

\newpage
\section{\href{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{Theory of Ordinary Differential Equations (Coddington Levinson)}}

\subsection{Chapter 11: Algebraic properties of linear boundary-value problems on a finite interval}

\subsubsection{Introduction}
\begin{defn}
Let $L$ be the \href[page=296]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{linear differential operator of oder $n$ ($n\geq 1$)}} defined by
\[Lx = p_0x^{(n)} + p_1x^{(n-1)} + \cdots + p_{n-1}x' + p_nx\]
where the $p_k$ are complex-valued functions of class $C^{n-k}$ on a closed bounded interval $[a,b]$ \LIN{(i.e., derivatives $p_k, p'_k, \ldots, p^{(n-k)}_k$ exist on $[a,b]$ and are continuous)} and $p_0(t)\neq 0$ on $[a,b]$.
\end{defn}

\begin{defn}
    \href[page=296]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{Homogeneous boundary conditions}} refer to a set of equations/constraints of the type
\begin{equation}\label{eq:homogeneous boundary conditions}
    \sum_{k=1}^n (M_{jk}x^{(k-1)}(a) + N_{jk}x^{(k-1)}(b))=0 \quad (j=1,\ldots,m) 
\end{equation}
where $M_{jk}, N_{jk}$ are complex constants.
\end{defn}

\begin{defn}
    A \href[page=296]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{homogeneous boundary-value problem}} concerns finding the solutions of 
    \[Lx=0\] on $[a,b]$ which satisfy some homogeneous boundary conditions defined above.
\end{defn}

\begin{defn}
    For any homogeneous boundary value problem, an \href[page=296]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{adjoint problem}} refers to the problem of finding the solutions of
    \[L^{+}x:=(-1)^n(\bar{p}_0x)^{(n)}+(-1)^{n-1}(\bar{p}_1x)^{(n-1)}+\cdots+\bar{p}_nx=0\]
    on $[a,b]$ which satisfy some homogeneous boundary conditions ``complementary'' to the conditions associated with the solutions of $Lx=0$.
\end{defn}

\begin{thm}{\href[page=296]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{(Green's formula)}}\label{thm:green's formula}
    For $u, v\in C^n$ on $[a,b]$,
    \DAS{Think about why we are asking for so much smoothness as $C^n$. Can you get away with any less? Consider looking up functions of bounded variation or absolutely continuous functions.}
    \LIN{We need the ($n-1$)st derivative of $u$, $v$ to exist for the form $[uv]$ to be defined. We need the $n$th derivatives to exist for $Lu$, $L^+v$ to be defined. Note that $u, v\in C^{n-1}$ ensures that the $(n-1)$st derivatives exist and are continuous, but not that the $n$th derivatives exist. \unsure[inline]{Do we need the $n$th derivatives to be continuous though?}}
    \LIN{By Proposition \ref{prop:(Lu,v)=(u,L^+v)}, f $u, v$ satisfy the corresponding boundary conditions, the equation below would be zero.}
    \begin{equation}\label{eq:green's formula}
        \int_{t_1}^{t_2}(Lu)\bar{v}\,dt - \int_{t_1}^{t_2}u(\overline{L^+v})\,dt = [uv](t_2) - [uv](t_1) 
    \end{equation}
    where $a\leq t_1<t_2\leq b$ and $[uv](t)$ is the form in $(u, u', \ldots, u^{(n-1)})$ and $(v, v', \ldots, v^{(n-1)})$ given by
    \begin{equation}\label{eq:[uv](t) defn}
        [uv](t)=\sum_{m=1}^n\sum_{j+k=m-1}(-1)^j u^{(k)}(t)(p_{n-m}\bar{v})^{(j)}(t)
    \end{equation}
\end{thm}
\begin{rmk}
    \href[page=297]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{Alternatively}, $[uv](t)$ can be written as \todo[inline]{Find out an explicit way to write $B_{jk}$ in terms of $p_{n-m}$. I'm thinking of deducing a formula for $B_{jk}$ by checking $n=2, 3, 4$, or implementing a program in Julia to figure out the expression for $B_{jk}$.}
    \begin{equation}\label{eq:[uv](t) in B matrix}
        \begin{split}
            [uv](t) &= \sum_{m=1}^n\sum_{j+k=m-1}(-1)^j u^{(k)}(t)(p_{n-m}\bar{v})^{(j)}(t)\\
            &= \sum_{m=1}^n\sum_{j+k=m-1}(-1)^j u^{(k)}(t)\left(\sum_{l=0}^j\binom{j}{l}p_{n-m}^{(j-l)}(t)\bar{v}^{(l)}(t)\right)\\
            &= \cdots ?\\
            &= \sum_{j,k=1}^n B_{jk}(t)u^{(k-1)}(t)\bar{v}^{(j-1)}(t)
        \end{split}
    \end{equation}
        where $B_{jk}$ may form the $j, k$-entry of the $n\times n$ matrix
    \begin{equation}\label{B(t)}
        B(t)=\begin{bmatrix}
            B_{11} & B_{12} & \cdots & \cdots & p_0(t)\\
            \vdots & \vdots & \cdots & -p_0(t) & 0\\
            (-1)^{n-1}p_0(t) & 0 & \cdots & 0 & 0
        \end{bmatrix}.
    \end{equation}
    Since $B(t)$ is square with $\det B(t)=(p_0(t))^n$ where $p_0(t)\neq 0$ on $[a,b]$ (as in the definition of $L$), $B(t)$ is nonsingular/invertible for $t\in [a,b]$.
\end{rmk}

\begin{defn}\label{defn:f cdot g}
    For vectors $f=(f_1,\ldots,f_k)$, $g=(g_1,\ldots,g_k)$, define the product
    \[f\cdot g:=\sum_{i=1}^k f_i\bar{g}_i.\]
    Note that $f\cdot g = g^*f$.
\end{defn}

\begin{defn}
    A \href[page=297]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{semibilinear form}} is a complex-valued function $\mathcal{S}$ defined for pairs of vectors $f=(f_1,\ldots,f_k)$, $g=(g_1,\ldots,g_k)$ satisfying
    \begin{align*}
        \mathcal{S}(\alpha f+\beta g, h)&=\alpha\mathcal{S}(f,h) + \beta\mathcal{S}(g,h)\\
        \mathcal{S}(f, \alpha g + \beta h) &= \bar{\alpha}\mathcal{S}(f,g) + \bar{\beta}\mathcal{S}(f, h)
    \end{align*}
    for any complex numbers $\alpha, \beta$ and vectors $f,g,h$.

    \LIN{Note that $\mathcal{S}$ is linear in the first argument but not the second one. If $\mathcal{S}$ were bilinear, it would be linear in each argument.}
	\DAS{Indeed, because it is semilinear in the second argument, it is also called ``sesquilinear'' (Latin for one and a half is sesquus).}
\end{defn}
\begin{rmk}
    If
    \[S = \begin{bmatrix}
        s_{11} & \cdots & s_{1k}\\
        \vdots &  & \vdots\\
        s_{k1} & \cdots & s_{kk}
    \end{bmatrix},\]
    then $Sf\cdot g$ is a semibilinear form
    \LIN{    
    \begin{equation}\label{eq:semibilinear form}
        \begin{split}
        \mathcal{S}(f,g) &:= Sf\cdot g\\
        &= \begin{bmatrix}
            s_{11} & \cdots & s_{1k}\\
            \vdots & & \vdots\\
            s_{k1} & \cdots & s_{kk}
        \end{bmatrix} \begin{bmatrix}
            f_1\\
            \vdots\\
            f_k
        \end{bmatrix} \cdot \begin{bmatrix}
            g_1\\
            \vdots\\
            g_k
        \end{bmatrix}\\
        &= \begin{bmatrix}
            \sum_{j=1}^k s_{1j}f_j\\
            \vdots\\
            \sum_{j=1}^k s_{kj}f_j
        \end{bmatrix}\cdot \begin{bmatrix}
            g_1\\
            \vdots\\
            g_k
        \end{bmatrix}\\
        &= \sum_{i=1}^k\left(\sum_{j=1}^k s_{ij}f_j\right)\bar{g}_i\\
        &=\sum_{i,j=1}^k s_{ij}f_i\bar{g}_i. 
        \end{split}
    \end{equation}
    Indeed:
    \begin{align*}
        \mathcal{S}(\alpha f+\beta g, h)
        &= \sum_{i,j=1}^k s_{ij}(\alpha f_j + \beta g_j)\bar{h}_i\\
        &= \alpha \sum_{i,j=1}^k s_{ij}f_j\bar{h}_i + \beta \sum_{i,j=1}^k g_j\bar{h}_i\\
        &= \alpha Sf\cdot h + \beta Sg\cdot h\\
        &= \alpha\mathcal{S}(f,h) + \beta\mathcal{S}(g,h).
    \end{align*}
    Similarly,
    \begin{align*}
        \mathcal{S}(f, \alpha g + \beta h)
        &= \sum_{i,j=1}^k s_{ij}f_j(\bar{\alpha g_i + \beta h_i})\\
        &= \bar{\alpha}\sum_{i,j=1}^k s_{ij}f_j\bar{g}_i + \bar{\beta}\sum_{i,j=1}^k f_j \bar{h}_i\\
        &= \bar{\alpha}Sf\cdot g + \bar{\beta}Sf\cdot h\\
        &= \bar{\alpha}\mathcal{S}(f,g) + \bar{\beta}\mathcal{S}(f,h).
    \end{align*}
    }
\end{rmk}

\begin{rmk}
    Under a similar matrix framework, we see that $[uv](t)$ is a semibilinear form with matrix $B(t)$:

    \LIN{
    Let $\vec{u}=(u, u', \ldots, u^{(n-1)})$ and $\vec{v}=(v, v', \ldots, v^{(n-1)})$. Then we have
    \begin{equation}\label{[uv](t) in semibilinear form}
        \begin{split}
        [uv](t) &= \sum_{j,k=1}^n B_{jk}(t)u^{(k-1)}(t)\bar{v}^{(j-1)}(t)\quad\mbox{(by \eqref{eq:[uv](t) in B matrix})}\\
        &= \sum_{i,j=1}^n (B_{ij}u^{(j-1)}\overline{v}^{(i-1)})(t)\\
        &= (B\vec{u}\cdot \vec{v})(t)\\
        &= \mathcal{S}(\vec{u},\vec{v})(t).
        \end{split}
    \end{equation}
    With this notation, we can rewrite the right hand side of Green's formula as a semibilinear form below:
    \begin{equation}\label{eq:green's formula in semibilinear form}
        \begin{split}
        [uv](t_2)-[uv](t_1) &= \sum_{j,k=1}^n B_{jk}(t_2)u^{(k-1)}(t_2)\bar{v}^{(j-1)}(t_2) - \sum_{j,k=1}^n B_{jk}(t_2)u^{(k-1)}(t_1)\bar{v}^{(j-1)}(t_1)\\
        &= B(t_2)\vec{u}(t_2)\cdot \vec{v}(t_2) - B(t_1)\vec{u}(t_1)\cdot \vec{v}(t_1)\\
        &= \begin{bmatrix}
            B_{11}(t_2) & \cdots & B_{1n}(t_2)\\
            \vdots &  & \vdots\\
            B_{n1}(t_2) & \ldots & B_{nn}(t_2)
        \end{bmatrix} 
        \begin{bmatrix}
        u(t_2)\\
        \vdots\\
        u^{(n-1)}(t_2)
        \end{bmatrix}\cdot 
        \begin{bmatrix}
            \bar{v}(t_2)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_2)
        \end{bmatrix} -\\
        &\qquad \begin{bmatrix}
            B_{11}(t_1) & \cdots & B_{1n}(t_1)\\
            \vdots &  & \vdots\\
            B_{n1}(t_1) & \ldots & B_{nn}(t_1)
        \end{bmatrix} 
        \begin{bmatrix}
        u(t_1)\\
        \vdots\\
        u^{(n-1)}(t_1)
        \end{bmatrix}\cdot 
        \begin{bmatrix}
            \bar{v}(t_1)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_1)
        \end{bmatrix}\\
        &= \begin{bmatrix}
            -B_{11}(t_1) & \cdots & -B_{1n}(t_1)\\
            \vdots &  & \vdots\\
            -B_{n1}(t_1) & \ldots & -B_{nn}(t_1)
        \end{bmatrix} 
        \begin{bmatrix}
        u(t_1)\\
        \vdots\\
        u^{(n-1)}(t_1)
        \end{bmatrix}\cdot 
        \begin{bmatrix}
            \bar{v}(t_1)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_1)
        \end{bmatrix} + \\
        &\qquad \begin{bmatrix}
            B_{11}(t_2) & \cdots & B_{1n}(t_2)\\
            \vdots &  & \vdots\\
            B_{n1}(t_2) & \ldots & B_{nn}(t_2)
        \end{bmatrix} 
        \begin{bmatrix}
        u(t_2)\\
        \vdots\\
        u^{(n-1)}(t_2)
        \end{bmatrix}\cdot 
        \begin{bmatrix}
            \bar{v}(t_2)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_2)
        \end{bmatrix}\\
        &= \begin{bmatrix}
            -B_{11}(t_1) & \cdots & -B_{1n}(t_1) & 0 & \cdots & 0\\
            \vdots &  & \vdots & \vdots &  & \vdots\\
            -B_{n1}(t_1) & \cdots & -B_{nn}(t_1) & 0 & \cdots & 0\\
            0 & \cdots & 0 & B_{11}(t_2) & \cdots & B_{1n}(t_2)\\
            \vdots &  & \vdots & \vdots &  & \vdots\\
            0 & \cdots & 0 & B_{n1}(t_2) & \cdots & B_{nn}(t_2)
        \end{bmatrix} 
        \begin{bmatrix}
            u(t_1)\\
            \vdots\\
            u^{(n-1)}(t_1)\\
            u(t_2)\\
            \vdots\\
            u^{(n-1)}(t_2)
        \end{bmatrix}\cdot
        \begin{bmatrix}
            \bar{v}(t_1)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_1)\\
            \bar{v}(t_2)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_2)
        \end{bmatrix}\\
        &= \begin{bmatrix}
            -B(t_1) & 0_n\\
            0_n & B(t_2)
        \end{bmatrix}
        \begin{bmatrix}
            u(t_1)\\
            \vdots\\
            u^{(n-1)}(t_1)\\
            u(t_2)\\
            \vdots\\
            u^{(n-1)}(t_2)
        \end{bmatrix}\cdot
        \begin{bmatrix}
            \bar{v}(t_1)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_1)\\
            \bar{v}(t_2)\\
            \vdots\\
            \bar{v}^{(n-1)}(t_2)
        \end{bmatrix}\\
        &=:\hat{B}
        \begin{bmatrix}
            \vec{u}(t_1)\\
            \vec{u}(t_2)
        \end{bmatrix}\cdot
        \begin{bmatrix}
            \vec{v}(t_1)\\
            \vec{v}(t_2)
        \end{bmatrix}.
    \end{split}
    \end{equation}
    }
    Since \href[page=298]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{$\det\hat{B}=(-1)^n\det B(t_1)\det B(t_2)$} \LIN{ (note that $\det(\lambda A) = \lambda^n\det(A)$ for $n\times n$ matrix $A$. In this case, $\det(-B(t_1)) = (-1)^n\det(B(t_1))$ since $B(t_1)$ is $n\times n$)}, $\hat{B}$ is nonsingular for $t_1, t_2\in [a,b]$ \LIN{(since $B(t)$ is nonsingular for $t\in [a,b]$, as shown before)}.
    %\unsure{Why the $(-1)^n$?}
    %\DAS{The $(-1)^n$ comes fromt he fact that $n$ rows of $\hat{B}$ have been multiplied by $-1$, compared to $\hat{B}$ being defined as the block diagonal matrix $\begin{bmatrix}B(t_1)&0\\0&B(t_2)\end{bmatrix}$. You can check the standard formula for the determinant of a block diagonal matrix in (for example) the matrix cookbook.}
\end{rmk}

\subsubsection{Boundary form formula}
\begin{defn}\label{defn:boundary form}
    Given any set of $2mn$ complex constants $M_{ij}, N_{ij}$ ($i=1,\ldots, m;\,j=1,\ldots,n$), define $m$ \href[page=298]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{boundary operators (boundary forms)}} $U_1,\ldots,U_m$ for functions $x$ on $[a,b]$, for which $x^{(j)}$ ($j=1,\ldots,n-1$) exists at $a$ and $b$, by
    \begin{equation}\label{eq:U_i defn}
        U_i x = \sum_{j=1}^n (M_{ij}x^{(j-1)}(a) + N_{ij}x^{(j-1)}(b))\quad\mbox{($i=1,\ldots,m$)} 
    \end{equation}
    $U_i$ are \textbf{linearly independent} if the only set of complex constants $c_1, \ldots, c_m$ for which
    \[\sum_{i=1}^m c_i U_ix=0\]
    for all $x\in C^{n-1}$ on $[a,b]$ is $c_1=c_2=\cdots =c_m=0$.
\end{defn}
\begin{rmk}
    Note that for $\alpha, \beta\in\C$ and $x_1, x_2\in C^{n-1}$ on $[a,b]$,
    \LIN{
    \begin{align*}
        U_i(\alpha x_1+\beta x_2) &= \sum_{j=1}^n (M_{ij}(\alpha x_1 + \beta x_2)^{(j-1)}(a) + N_{ij}(\alpha x_1 + \beta x_2)^{(j-1)}(b))\\
        &= \alpha \sum_{j=1}^n (M_{ij}x_1^{(j-1)}(a) + N_{ij}x_1^{(j-1)}(b)) +\\
        &\qquad \beta \sum_{j=1}^n (M_{ij}x_2^{(j-1)}(a) + N_{ij}x_2^{(j-1)}(b))\quad\mbox{(by linearity of derivatives)}\\
        &= \alpha U_i x_1 + \beta U_i x_2.
    \end{align*}
    }
    So $U_i$ are linear operators.
\end{rmk}

\begin{rmk}\label{rmk:writing Ux in M, N (M:N)}
    To describe \eqref{eq:U_i defn} with matrices, define
    \[
        \xi:= \begin{bmatrix}x\\ x'\\ \vdots \\ x^{(n-1)}
        \end{bmatrix};
            \quad
        U := \begin{bmatrix}U_1\\ U_2\\ \vdots \\ U_m
        \end{bmatrix};
            \quad
        M := \begin{bmatrix}
            M_{11} & \cdots & M_{1n}\\
            \vdots &  & \vdots\\
            M_{m1} & \cdots & M_{mn}
        \end{bmatrix};
        \quad
        N := \begin{bmatrix}
            N_{11} & \cdots & N_{1n}\\
            \vdots &  & \vdots\\
            N_{m1} & \cdots & N_{mn}
        \end{bmatrix}.
    \]
    Then \eqref{eq:U_i defn} can be written as
    \[Ux = M\xi(a) + N\xi(b).\]
    \LIN{
    Indeed:
    \begin{align*}
        M\xi(a) + N\xi(b) &= \begin{bmatrix}
            M_{11} & \cdots & M_{1n}\\
            \vdots &  & \vdots\\
            M_{m1} & \cdots & M_{mn}
        \end{bmatrix}\begin{bmatrix}x(a)\\ x'(a)\\ \vdots \\ x^{(n-1)}(a)
            \end{bmatrix} + \begin{bmatrix}
                N_{11} & \cdots & N_{1n}\\
                \vdots &  & \vdots\\
                N_{m1} & \cdots & N_{mn}
            \end{bmatrix}\begin{bmatrix}x(b)\\ x'(b)\\ \vdots \\ x^{(n-1)}(b)
            \end{bmatrix}\\
            &= \begin{bmatrix}
                \sum_{j=1}^n M_{1j}x^{(j-1)}(a)\\
                \vdots\\
                \sum_{j=1}^n M_{mj}x^{(j-1)}(a)
            \end{bmatrix} + \begin{bmatrix}
                \sum_{j=1}^n N_{1j}x^{(j-1)}(b)\\
                \vdots\\
                \sum_{j=1}^n N_{mj}x^{(j-1)}(b)
            \end{bmatrix}\\
            &= \begin{bmatrix}
                \sum_{j=1}^n (M_{1j}x^{(j-1)}(a) + N_{1j}x^{(j-1)}(b))\\
                \vdots\\
                \sum_{j=1}^n (M_{mj}x^{(j-1)}(a) + N_{mj}x^{(j-1)}(b))
            \end{bmatrix}\\
            &= \begin{bmatrix}
                U_1 x\\
                \vdots\\
                U_m x
            \end{bmatrix} = \begin{bmatrix}U_1\\ U_2\\ \vdots \\ U_m
            \end{bmatrix}x = Ux.
    \end{align*}
    }
    Define the $m\times 2n$ matrix
    \[(M:N):=\begin{bmatrix}
        M_{11} & \cdots & M_{1n} & N_{11} & \cdots & N_{1n}\\
        \vdots &  & \vdots & \vdots & & \vdots\\
        M_{m1} & \cdots & M_{mn} & N_{m1} & \cdots & N_{mn}
    \end{bmatrix}.\]
    Then $U_1,\ldots,U_m$ are linearly independent if and only if $\rank(M:N)=m$, or equivalently, $\rank(U)=m$. 
    \LIN{
    Recall that the rank of a matrix is the largest number of linearly independent rows or columns in it. For a matrix $A_{m\times n}$, $\rank(A)\leq \min\{m,n\}$ and $\rank(A)=\rank(A^T)$.

    $Ux$ can be written as
    \begin{align*}
        Ux &= \begin{bmatrix}
            \sum_{j=1}^n (M_{1j}x^{(j-1)}(a) + N_{1j}x^{(j-1)}(b))\\
            \vdots\\
            \sum_{j=1}^n (M_{mj}x^{(j-1)}(a) + N_{mj}x^{(j-1)}(b))
        \end{bmatrix}\\
        &= \begin{bmatrix}
            M_{11} & \cdots & M_{1n} & N_{11} & \cdots & N_{1n}\\
            \vdots &  & \vdots & \vdots & & \vdots\\
            M_{m1} & \cdots & M_{mn} & N_{m1} & \cdots & N_{mn}
        \end{bmatrix} \begin{bmatrix}x(a)\\\vdots\\x^{(n-1)}(a)\\ x(b)\\\vdots\\x^{(n-1)}(b)\end{bmatrix}\\
        &= (M:N)\begin{bmatrix}
            \xi(a)\\
            \xi(b)
        \end{bmatrix}.
    \end{align*}
    }
\end{rmk}

\begin{defn}\label{defn:complementary boundary form}
    If $U=(U_1,\ldots, U_m)$ is any boundary form with $\rank(U)=m$ and $U_c=(U_{m+1},\ldots,U_{2n})$ any form with $\rank(U_c)=2n-m$ such that $(U_1,\ldots, U_{2n})$ has rank $2n$, then $U$ and $U_c$ are \href[page=299]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{complementary boundary forms}}. ``Adjoining'' $U_{m+1},\ldots, U_{2n}$ to $U_1,\ldots,U_m$ is equivalent to imbedding the matrix $(M:N)$ in a $2n\times 2n$ nonsingular matrix \LIN{(recall that for square matrices, nonsingular $\iff$ full rank)}.
\end{defn}

We wish to describe the right hand side of Green's formula \eqref{eq:green's formula} as a linear combination of a boundary form $U$ and a complementary form $U_c$. To do so, we consider the following results about the semibilinear form \eqref{eq:semibilinear form}.

\begin{defn}
    For a matrix $A=(a_{ij})$, its \href[page=299]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{adjoint}} is defined as the conjugate transpose $A^* = (\bar{a}_{ij})$.
\end{defn}
\begin{prop}
    In the context of the semibilinear form \eqref{eq:semibilinear form}, we have
    \begin{equation}\label{eq:semibilinear adjoint}
        Sf\cdot g = f\cdot S^*g. 
    \end{equation}
\end{prop}
\LIN{
    \begin{proof}
        \begin{align*}
            Sf\cdot g &= \sum_{i,j=1}^k s_{ij}f_j\bar{g}_i \quad\mbox{(by \eqref{eq:semibilinear form})};\\
            f\cdot S^*g &= \begin{bmatrix}
                f_1\\
                \vdots\\
                f_k
            \end{bmatrix}\cdot \begin{bmatrix}
                \bar{s}_{11} & \cdots & \bar{s}_{k1}\\
                \vdots & & \vdots\\
                \bar{s}_{1k} & \cdots & \bar{s}_{kk}
            \end{bmatrix} \begin{bmatrix}
                g_1\\
                \vdots\\
                g_k
            \end{bmatrix}\\
            &= \begin{bmatrix}
                f_1\\
                \vdots\\
                f_k
            \end{bmatrix}\cdot \begin{bmatrix}
                \sum_{j=1}^k \bar{s}_{j1}g_j\\
                \vdots\\
                \sum_{j=1}^k \bar{s}_{jk}g_j
            \end{bmatrix}\\
            &= \sum_{i=1}^k f_i\cdot \overline{\left(\sum_{j=1}^k\bar{s}_{ji}g_j\right)}\\
            &= \sum_{i=1}^k f_i\cdot \left(\sum_{j=1}^k s_{ji}\bar{g}_j\right)\\
            &= \sum_{i,j=1}^k s_{ji}f_i\bar{g}_j = Sf\cdot g.
        \end{align*}
    \end{proof}
}

\begin{prop}\label{prop:unique nonsingular G in semibilinear form}
    Let $\mathcal{S}$ be the semibilinear form associated with a nonsingular matrix $S$. Suppose $\bar{f}:=Ff$ where $F$ is a nonsingular matrix. Then there exists a unique nonsingular matrix $G$ such that if $\bar{g}=Gg$, then $\mathcal{S}(f,g)=\bar{f}\cdot \bar{g}$ for all $f, g$.
\end{prop}
\LIN{
\begin{proof}
    Let $G:=(SF^{-1})^*$, then
    \begin{align*}
        \mathcal{S}(f,g) &= Sf\cdot g\\
        &= S(F^{-1}F)f\cdot g\\
        &= SF^{-1}(Ff)\cdot g\\
        &= SF^{-1}\bar{f} \cdot g\\
        &= \bar{f}\cdot (SF^{-1})^*g \quad\mbox{(by \eqref{eq:semibilinear adjoint})}\\
        &= \bar{f}\cdot G*g\\
        &= \bar{f}\cdot \bar{g}.
    \end{align*}
    To see that $G$ is nonsingular, note that $\det G = \det((\overline{SF^{-1}})^T) = \det(\overline{SF^{-1}}) = \overline{\det(SF^{-1})} = \overline{\det(S)\det(F)^{-1}}\neq 0$ since $S, F$ are nonsingular.
\end{proof}
}

\begin{prop}\label{prop:last k-j components linear combination}
    Suppose $\mathcal{S}$ is associated with the unit matrix $E$, i.e., $\mathcal{S}(f,g)=f\cdot g$. Let $F$ be a nonsingular matrix such that the first $j$ ($1\leq j<k$) components of $\bar{f}=Ff$ are the same as those of $f$. Then the unique nonsingular matrix $G$ such that $\bar{g}=Gg$ and $\bar{f}\cdot \bar{g}=f\cdot g$ (as in Proposition \ref{prop:unique nonsingular G in semibilinear form}) is such that the last $k-j$ components of $\bar{g}$ are linear combinations of the last $k-j$ components of $g$ with nonsingular coefficient matrix.
\end{prop}
\LIN{
\begin{proof}
    We note that for the condition on $F$ to hold, $F$ must have the form
\[\begin{bmatrix}E_j & 0_+\\
F_+ & F_{k-j}\end{bmatrix}_{k\times k}\]
where $E_j$ is the $j\times j$ identity matrix, $0_+$ is the $j\times (k-j)$ zero matrix, $F_+$ is a $(k-j)\times j$ matrix, and $F_{k-j}$ a $(k-j)\times (k-j)$ matrix. Let $G$ be the unique nonsingular matrix in Proposition \ref{prop:unique nonsingular G in semibilinear form}. Write $G$ as
\[\begin{bmatrix}G_j & G_-\\
G_= & G_{k-j}\end{bmatrix}_{k\times k}\]
where $G_j, G_-, G_=, G_{k-j}$ are $j\times j, j\times (k-j), (k-j)\times j, (k-j)\times (k-j)$ matrices, respectively. By the definition of $G$,
\[f\cdot g = Ff\cdot Gg = \bar{f}\cdot Gg = G^*\bar{f}\cdot g = G^*Ff\cdot g,\]
(where the third equality follows from a reverse application of \eqref{eq:semibilinear adjoint} with $\bar{f}$ as $f$, $G^*$ as $S$) which implies
\[G^*F = E_k.\]
Since
\begin{align*}
    G^*F &= \begin{bmatrix}
        G^*_j & G^*_=\\
        G^*_- & G^*_{k-j}
    \end{bmatrix}\begin{bmatrix}E_j & 0_+\\
    F_+ & F_{k-j}\end{bmatrix}\\
    &= \begin{bmatrix}
        G^*_j + G^*_= F_+ & G^*_= F_{k-j}\\
        G^*_- + G^*_{k-j} F_+ & G^*_{k-j}F_{k-j}
    \end{bmatrix}\\
    &= \begin{bmatrix}
        E_j & 0_{j\times (k-j)}\\
        0_{(k-j)\times j} & E_{k-j}
    \end{bmatrix}.
\end{align*}
Thus, $G^*_=F_{k-j}=0_+$, the $j\times (k-j)$ zero matrix. But $\det F = \det(E_j)\cdot \det(F_{k-j})\neq 0$, so $\det F_{k-j}\neq 0$ and we must have $G^*_==0_+$, i.e., $G_= =0_{(k-j)\times j}$. Thus, $G$ is upper-triangular, and so $\det G = \det G_j \cdot \det G_{k-j}\neq 0$, which implies $\det G_{k-j}\neq 0$ and $G_{k-j}$ is nonsingular. Hence,
\[\bar{g} = Gg = \begin{bmatrix}G_j & G_-\\
0_{(k-j)\times j} & G_{k-j}\end{bmatrix} \begin{bmatrix}
g_1\\
\vdots\\
g_k
\end{bmatrix}\]
where $G_{k-j}$ is the nonsingular coefficient matrix such that
\[\begin{bmatrix}
    \bar{g}_{j-1}\\
    \vdots\\
    \bar{g}_{k}
\end{bmatrix} = G_{k-j}\begin{bmatrix}
    g_{j-1}\\
    \vdots\\
    g_{k}
\end{bmatrix}.\]
\end{proof}
}


\begin{thm}\href[page=300]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{(Boundary-form formula)}\label{thm:boundary form formula}
    Given any boundary form $U$ of rank $m$ (Definition \ref{defn:boundary form}), and any complementary form $U_c$ (Definition \ref{defn:complementary boundary form}), there exist unique boundary forms $U_c^+$, $U^+$ of rank $m$ and $2n-m$, respectively, such that
    \begin{equation}\label{eq:boundary form formula}
        [xy](b)-[xy](a) = Ux\cdot U_c^+y + U_{c}x\cdot U^+y. %\quad\mbox{\DAS{typo}}
    \end{equation}
    If $\tilde{U}_c$ is any other complementary form to $U$, and $\tilde{U}^+_c, \tilde{U}^+$ the corresponding forms of rank $m$ and $2n-m$, then
    \begin{equation}\label{eq:adjoint boundary forms unique up to linear transformation}
        \tilde{U}^+ y = C^*U^+y
    \end{equation}
    for some nonsingular matrix $C$.
\end{thm}
\begin{rmk}
    \LIN{This mean that, given a boundary form, its adjoint boundary forms are related to each other by linear transformation.}

	\DAS{Yes. It is a kind of uniqueness result. It says that the adjoint boundary forms are unique \emph{up to linear transformation}. This can be understood further as saying that you don't need to find the ``right'' adjoint boundary form; any adjoint boundary form is good enough because they are all, from the point of view of their action, the same object. In practice, we will usually aim to have our boundary forms in (some sense of) reduced row-echelon form for convenience and comparability. Theorem \ref{thm:boundary form formula} says that such a form is only canonical because it is something we have decided upon, not because it is mathematically better.}
\end{rmk}
\begin{proof}
    \LIN{
    Recall from \eqref{eq:green's formula in semibilinear form} that the left hand side of \eqref{eq:boundary form formula} can be considered as a semibilinear form $\mathcal{S}(f,g)=\hat{B}f\cdot g$ for vectors
    \[
        f=
        \begin{bmatrix}
            x(a)\\
            \vdots\\
            x^{(n-1)}(a)\\
            x(b)\\
            \vdots\\
            x^{(n-1)}(b)
        \end{bmatrix},\,
        g=
        \begin{bmatrix}
            y(a)\\
            \vdots\\
            y^{(n-1)}(a)\\
            y(b)\\
            \vdots\\
            y^{(n-1)}(b)
        \end{bmatrix}
    \]
    with the nonsingular matrix
    \[
        \hat{B}=
        \begin{bmatrix}
            -B(a) & 0_n\\
            0_n & B(b)
        \end{bmatrix}.
    \]
    Recall from Remark \ref{rmk:writing Ux in M, N (M:N)} that 
    \[Ux = M\xi(a) + N\xi(b) = (M:N)\begin{bmatrix}\xi(a)\\ \xi(b)\end{bmatrix}\]
    for $M, N, \xi$ are as defined there. With the definition of $f$, we have $f=\begin{bmatrix}\xi(a)\\ \xi(b)\end{bmatrix}$ and thus
    \[Ux = (M:N)f.\]
    By Definition \ref{defn:complementary boundary form}, $U_c x = (\tilde{M}:\tilde{N})f$ for two appropriate matrices $\tilde{M}, \tilde{N}$ for which
    \[H = \begin{bmatrix}M & N\\
    \tilde{M} & \tilde{N}\end{bmatrix}_{2n\times 2n}\]
    has rank $2n$. Thus,
    \[\begin{bmatrix}Ux\\ U_cx\end{bmatrix} = \begin{bmatrix}(M:N)f\\(\tilde{M}:\tilde{N})f\end{bmatrix} = \begin{bmatrix}M & N\\
    \tilde{M} & \tilde{N}\end{bmatrix}f = Hf.\]
    By Proposition \ref{prop:unique nonsingular G in semibilinear form}, there exists a unique $2n\times 2n$ nonsingular matrix $J$ such that $\mathcal{S}(f,g) = Hf\cdot Jg$. Let $U^+, U_c^+$ be such that
    %\unsure[inline]{Is the direction correct? e.g., from the existence of $J$, construct $U^+$ and $U_c^+$.}
	%\DAS{I think that's right. Existence of $J$ gives you existence of $U^+$ and $U_c^+$.}
    \[Jg = \begin{bmatrix}U_c^+ y\\ U^+y\end{bmatrix},\]
    then \eqref{eq:boundary form formula} holds since
    \[[xy](b)-[xy](a)=\mathcal{S}(f,g) = Hf\cdot Jg = \begin{bmatrix}Ux\\ U_cx\end{bmatrix}\cdot\begin{bmatrix}U_c^+ y \\ U^+y\end{bmatrix} = Ux\cdot U_c^+y + U_cx\cdot U^+y.\]
    }
    The second statement in the theorem follows from Proposition \ref{prop:last k-j components linear combination} with $Hf$ and $Jg$ corresponding to $f$ and $g$.\unsure[inline]{Proposition \ref{prop:last k-j components linear combination} poses condition on $F$ and invokes the existence of $G$; what are the objects corresponding to $F$ and $G$ here?}
	\DAS{Not sure. It seems to me that unicity of $J$ is only once $H$ has been chosen, so a different $H$ (ie different complementary boundary form) will give you a different $J$, hence different adjoint boundary form and different complementary adjoint boundary form. So the task is to understand how the chain of changes new complementary boundary form $\to$ new $H$ $\to$ new $J$ $\to$ new adjoint boundary forms provides this linearity result. I guess Proposition \ref{prop:last k-j components linear combination} must help with this in some way.}
\end{proof}

\subsubsection{Homogeneous Boundary-value Problems and Adjoint Problems}

\begin{defn}
    For any boundary form $U$ of rank $m$ there is associated the \href[page=300]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{homogeneous boundary condition}}
    \begin{equation}\label{eq:homogeneous boundary condition}
        Ux=0
    \end{equation}
    for functions $x\in C^{n-1}$ on $[a,b]$. If $U^+$ is any boundary form of rank $2n-m$ determined as in Theorem \ref{thm:boundary form formula}, then the homogeneous boundary condition
    \begin{equation}\label{eq:adjoint boundary condition}
        U^+x=0
    \end{equation}
    is an \href[page=301]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{adjoint boundary condition}} to \ref{eq:homogeneous boundary condition}.
\end{defn}

\begin{prop}\label{prop:(Lu,v)=(u,L^+v)}
    By Green's formula \eqref{eq:green's formula} and the boundary-form formula \eqref{eq:boundary form formula}, for $(u,v):=\int_a^b u\bar{v}\,dt$, 
    \[(Lu, v) = (u, L^+v)\]
    for all $u\in C^n$ on $[a,b]$ satisfying \eqref{eq:homogeneous boundary condition} and all $v\in C^n$ on $[a,b]$ satisfying \eqref{eq:adjoint boundary condition}.
\end{prop}
\LIN{
\begin{proof}
    \begin{equation}
        \begin{split}
            (Lu, v) - (u, L^+v) &= \int_a^b Lu\bar{v}\,dt - \int_a^b u(\overline{L^+v})\,dt\\
            &= [uv](a) - [uv](b) \quad\mbox{(by Green's formula \eqref{eq:green's formula})}\\
            &= Uu\cdot U_c^+ v + U_c u\cdot U^+v \quad\mbox{(by boundary-form formula \eqref{eq:boundary form formula})}\\
            &= 0\cdot U_c^+ v + U_c u \cdot 0 \quad\mbox{(by \eqref{eq:homogeneous boundary condition} and \eqref{eq:adjoint boundary condition})}\\
            &= 0.
        \end{split}
    \end{equation}
\end{proof}
}

\begin{rmk}\label{rmk:uniqueness of U^+}
    Let $D, D^+$ be the set of functions $u\in C^n$ satisfying \eqref{eq:homogeneous boundary condition} and \eqref{eq:adjoint boundary condition}, respectively. Then Theorem \ref{thm:boundary form formula} shows that $D^+$ is uniquely determined by $U$, although $U^+$ is not.
    %\unsure[inline]{$U^+$ is not uniquely determined by $U$ but by $U$ and $U_c$. But $D^+$ is the set of functions $x$ satisfying $U^+x=0$; if $U^+$ is not uniquely determined by $U$, how can $D^+$ be?}
    %\DAS{Apply \eqref{eq:adjoint boundary forms unique up to linear transformation} to equation \eqref{eq:adjoint boundary condition}. Because the RHS of equation \eqref{eq:adjoint boundary condition} is $0$ and $C^\star$ is linear, we still get $0$.}
    \LIN{ Note that $U^+$ is not uniquely determined by $U$ because adjoint boundary forms are only unique up to linear transformation. Yet $D^+$ is uniquely determined by $U$ because given $U$, its different adjoint boundary forms $U^+$ related to each other by linear transformations constitute the same condition on $D^+$: Since $\tilde{U}^+=C^*U^+$ for some linear map $C$, $\tilde{U}^+x=0$ $\iff$ $U^+x=0$.}
\end{rmk}

\LIN{
Just like how $U$ is associated with two $m\times n$ matrices $M, N$ (Remark \ref{rmk:writing Ux in M, N (M:N)}), $U^+$ is associated with two $n\times (2n-m)$ matrices $P, Q$ such that $(P^*:Q^*)$ has rank $2n-m$ and
\begin{equation}\label{eq:U^+x in P* Q*}
    U^+x = P^*\xi(a) + Q^*\xi(b)
\end{equation}
Note that imbedding $M, N, P^*, Q^*$ in the same matrix gives
\begin{align*}
    \begin{bmatrix}
        (M:N)_{m\times 2n}\\
        (P^*:Q^*)_{(2n-m)\times 2n}
    \end{bmatrix}_{2n\times 2n} & = \begin{bmatrix}
        M & N\\
        P^* & Q^*
    \end{bmatrix}
\end{align*}
is an $2n\times 2n$ matrix of full rank.
}

We want to characterize the adjoint condition \eqref{eq:adjoint boundary condition} in terms of the matrices $M, N, P, Q$.

\begin{thm}\label{thm:condition iff adjoint}
    The boundary condition $U^+x=0$ is adjoint to $Ux=0$ if and only if
    \begin{equation}\label{eq:condition iff adjoint}
        MB^{-1}(a)P = NB^{-1}(b)Q
    \end{equation}
    where $B(t)$ is the $n\times n$ matrix associated with the form $[xy](t)$ (\eqref{B(t)}).
\end{thm}
\begin{proof}
    \LIN{
    Let $\eta := (y, y', \ldots, y^{(n-1)})$,
    then $[xy](t)=B(t)\xi(t)\cdot \eta(t)$ by \eqref{[uv](t) in semibilinear form}.

    Suppose $U^+x=0$ is adjoint to $Ux=0$. By definition of adjoint boundary condition \ref{eq:adjoint boundary condition}, $U^+$ is determined as in Theorem \ref{thm:boundary form formula}. But by Theorem \ref{thm:boundary form formula}, in determining $U^+$, there exist boundary forms $U_c$, $U_c^+$ of rank $2n-m$ and $m$, respectively, such that \ref{eq:boundary form formula} holds. 

    Put
    \begin{align*}
        U_cx &= M_c\xi(a) + N_c\xi(b)\quad \rank(M_c:N_c) = 2n-m\\
        U_c^+y &= P_c^*\eta(a) + Q_c^*\eta(b)\quad\rank(P_c^*:Q_c^*)=m.
    \end{align*}
    Then by the boundary-form formula (\ref{eq:boundary form formula}),
    \begin{align*}
        B(b)\xi(b)\cdot \eta(b) - B(a)\xi(a)\cdot \eta(a) &= (M\xi(a) + N\xi(b))\cdot (P_c^*\eta(a) + Q_c^*\eta(b)) + \\
        &\qquad (M_c\xi(a) + N_c\xi(b))\cdot (P^*\eta(a) + Q^*\eta(b)).
    \end{align*}
    By \ref{eq:semibilinear adjoint}, 
    \[M\xi(a)\cdot P_c^*\eta(a) = P_cM\xi(a)\cdot \eta(a).\]
    Thus,
    \begin{align*}
        B(b)\xi(b)\cdot \eta(b) - B(a)\xi(a)\cdot \eta(a) &= (P_c M + PM_c)\xi(a)\cdot \eta(a) + (Q_cM + QM_c)\xi(a)\cdot \eta(b) \\
        &\qquad (P_cN + PN_c) \xi(b)\cdot \eta(a) + (Q_cN + QN_c) \xi(b)\cdot \eta(b).
    \end{align*}
    }
    Thus, we have
    \begin{align*}
        P_cM + PM_c &= - B(a) & P_cN + PN_c &= 0_n\\
        Q_cM + QM_c &= 0_n & Q_cN + QN_c &= B(b).
    \end{align*}
    Since $\det B(t)\neq 0$ on $t\in[a,b]$, $B^{-1}(a)$, $B^{-1}(b)$ exist, and thus
    \begin{align*}
        \begin{bmatrix}
            -B^{-1}(a)P_c & -B^{-1}(a)P\\
            B^{-1}(b)Q_c & B^{-1}(b)Q
        \end{bmatrix}
        \begin{bmatrix}
            M & N\\
            M_c & N_c
        \end{bmatrix}
        =
        \begin{bmatrix}
            E_n & 0_n\\
            0_n & E_n
        \end{bmatrix}.
    \end{align*}
    \LIN{
    Recall that $\begin{bmatrix}
        M & N\\
        M_c & N_c
    \end{bmatrix}$ has full rank, which means that it is nonsingular (Definition \ref{defn:complementary boundary form}). Thus, the two matrices on the left are inverses of each other. So we also have
    }
    \begin{align*}
        \begin{bmatrix}
            M & N\\
            M_c & N_c
        \end{bmatrix}
        \begin{bmatrix}
            -B^{-1}(a)P_c & -B^{-1}(a)P\\
            B^{-1}(b)Q_c & B^{-1}(b)Q
        \end{bmatrix}
        =
        \begin{bmatrix}
            E_m & 0_+\\
            0_- & E_{2n-m}
        \end{bmatrix}.
    \end{align*}
    Therefore,
    \[-MB^{-1}(a)P + NB^{-1}(b)Q = 0_+,\]
    which is \eqref{eq:condition iff adjoint}.

    Conversely, let $U_1^+$ is a boundary form of rank $2n-m$ such that
    \[U_1^+y = P_1^*\eta(a) + Q_1^*\eta(b)\]
    for appropriate $P_1^*$, $Q_1^*$ with $\rank(P_1^*:Q_1^*)=2n-m$. Suppose
    \begin{equation}\label{eq:condition iff adjoint converse}
        MB^{-1}(a)P_1 = NB^{-1}(b)Q_1
    \end{equation}
    holds.

    \LIN{
    Recall that $\dim(\text{solution space}) + \rank(\text{matrix}) = \text{$\#$ of unknown variables}$. Let $u$ be a $2n\times 1$ vector, then there exist exactly $2n-m$ linearly independent $2n\times 1$ vector solutions of the linear system $(M:N)_{m\times 2n}u=0$. By \eqref{eq:condition iff adjoint converse},
    \[MB^{-1}(a)P_1 - NB^{-1}(b)Q=0,\]
    and thus
    \[(M:N)_{m\times 2n}\begin{bmatrix}
        B^{-1}(a)P_1\\
        -B^{-1}(b)Q_1
    \end{bmatrix}_{2n\times (2n-m)} = 0_{m\times (2n-m)}.\]
    }
    So the $2n-m$ columns of the matrix
    \[H_1:= \begin{bmatrix}
        B^{-1}(a)P_1\\
        -B^{-1}(b)Q_1
    \end{bmatrix}\]
    are solutions of this system. Since $\rank(P_1^*:Q_1^*)=2n-m$,
    \[\rank\begin{bmatrix}P_1\\ Q_1\end{bmatrix}=2n-m.\]
    Since $B(a)$, $B(b)$ are nonsingular, $\rank(H_1)=2n-m$.

    \LIN{
    If $U^+x=P^*\xi(a) + Q^*\xi(b)=0$ is a boundary condition adjoint to $Ux=0$, then the matrix
    \[\begin{bmatrix}
        -B^{-1}(a)P_c & -B^{-1}(a)P\\
        B^{-1}(b)Q_c & B^{-1}(b)Q
    \end{bmatrix}_{2n\times 2n}\]
is nonsingular (because it has inverse $\begin{bmatrix}M & N\\ M_c & N_c\end{bmatrix}$), i.e., it has full rank. Thus, if 
    \[H = \begin{bmatrix}
        -B^{-1}(a)P\\
        B^{-1}(b)Q
    \end{bmatrix}_{n\times (2n-m)},\]
    then $\rank(H)=2n-m$. Therefore, by \eqref{eq:condition iff adjoint}, the $2n-m$ columns of $H$ also form $2n=m$ linearly independent solutions of $(M:N)u=0$, as in the case of $H_1$. Hence, there exists a nonsingular $(2n-m)\times (2n-m)$ matrix $A$ such that $H_1=HA$ (change of basis in the solution space). 
    }
    Thus we have
    \begin{align*}
        \begin{bmatrix}
            B^{-1}(a)P_1\\
            -B^{-1}(b)Q_1
        \end{bmatrix} = H_1 = HA = \begin{bmatrix}
            B^{-1}(a)PA\\
            -B^{-1}(b)QA
        \end{bmatrix},
    \end{align*}
    or $P_1=PA$, $Q_1=QA$. Thus, 
    \[U_1^+y = P_1^*\eta(a) + Q_1^*\eta(b) = A^*P^*\eta(a) + A^*Q^*\eta(b)= A^* U^+y.\]
    This implies that $U_1^+y=0$ is an adjoint boundary condition to $Ux=0$.\unsure[inline]{Is this by Theorem \ref{thm:boundary form formula}? But it says adjoint boundary forms are related by multiplication by nonsingular matrices, not that the multiplication of an adjoint boundary form by a nonsingular matrix is still an adjoint boundary form.}
\end{proof}

\begin{defn}
    If $U$ is a boundary form of rank $m$, the problem of finding solutions of
    \[\pi_m:\,Lx=0\quad Ux=0\]
    on $[a,b]$ is a \href[page=303]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{homogeneous boundary-value problem of rank $m$}}. The problem
    \[\pi_{2n-m}^+:\,L^+x=0\quad U^+x=0\]
    on $[a,b]$ is the \href[page=303]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{adjoint boundary-value problem to $\pi_m$}}.
    %\unsure[inline]{Given that $U^+$ is not uniquely determined by $U$, is the adjoint problem unique?} 
    %\DAS{That depends on what you mean by ``problem''. If you think of the problem as the pair $(L^+,U^+)$, then no it is not unique. But if you think of the problem as being ``the set of all functions $x$ in an appropriate space satisfying $L^+x=0$ and $U^+x=0$'', then yes the adjoint problem is unique. This is effectively the same statement as my comment on the non-unicity of $U^+$ above.} 
    
    \LIN{Note the ``the'' in the above statement. In the language of Remark \ref{rmk:uniqueness of U^+}, since $D^+$ is uniquely determined by $U$, the problem is unique in terms of its solutions, even though $U^+$ is not unique.}
    
    In fact, $\pi_m$ and $\pi_{2n-m}^+$ are adjoint problems to each other. The zero function on $[a,b]$ is a solution to both $\pi_m$ and $\pi_{2n-m}^+$, known as the \textbf{trivial solution}.
\end{defn}

\begin{thm}\label{thm:self-adjoint boundary condition}
    If $m=n$, the boundary condition $Ux=0$ is adjoint to itself if and only if
    \[MB^{-1}(a)M^* = NB^{-1}(b)N^*.\]
\end{thm}
\LIN{
\begin{proof}
    Replace $P, Q$ with $M, N$ in Theorem \ref{thm:condition iff adjoint}.
\end{proof}
}

\begin{thm}\label{thm:self-adjoint boundary-value problem}
    If $Ux=0$ is self-adjoint and $L^+=L$, the boundary-value problem $\pi_m$ is self-adjoint, i.e., if $u, v\in C^n$ on $[a,b]$ and satisfy $Ux=0$, then
    \[(Lu, v) = (u, Lv).\]
\end{thm}
\LIN{
\begin{proof}
    The equation follows as a special case of Proposition \ref{prop:(Lu,v)=(u,L^+v)}.
\end{proof}
}

\begin{defn}
    Let $\varphi_1,\ldots,\varphi_n$ be a fundamental set \LIN{(basis of the solution space to $Lx=0$)}. Let $\Phi$ denote the nonsingular matrix
    \[\Phi:=\begin{bmatrix}
        \varphi_1 & \cdots & \varphi_n\\
        \varphi_1' & \cdots & \varphi_n'\\
        \vdots &  & \vdots\\
        \varphi_1^{(n-1)} & \vdots & \varphi_n^{(n-1)}
    \end{bmatrix}.\]
    Then $\Phi$ is a \href[page=303]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{fundamental matrix associated with $Lx=0$}}. Similarly, if $\psi_1,\ldots,\psi_n$ is a fundamental set fo $L^+x=0$, then the corresponding fundamental matrix is
    \[\Psi:=\begin{bmatrix}
        \psi_1 & \cdots & \psi_n\\
        \psi_1' & \cdots & \psi_n'\\
        \vdots &  & \vdots\\
        \psi_1^{(n-1)} & \vdots & \psi_n^{(n-1)}
    \end{bmatrix}.\]
    The meanings of $U$, $U^+$ can be extended from vectors (Remark \ref{rmk:writing Ux in M, N (M:N)}) to matrices as follows:
    \begin{align*}
        U\Phi &:= M\Phi(a) + N\Phi(b)\\
        U^+\Psi &:= P^*\Psi(a) + Q^*\Psi(b).
    \end{align*}
\end{defn}

\LIN{
\begin{rmk}\label{rmk:U Phi}
    We note that
    \begin{align*}
        U\Phi &= M\Phi(a) + N\Phi(b)\\
        &= 
        \begin{bmatrix}
            M_{11} & \cdots & M_{1n}\\
            \vdots &  & \vdots\\
            M_{m1} & \cdots & M_{mn}
        \end{bmatrix}
        \begin{bmatrix}
            \varphi_1(a) & \cdots & \varphi_n(a)\\
            \varphi_1'(a) & \cdots & \varphi_n'(a)\\
            \vdots& & \vdots\\
            \varphi_1^{(n-1)}(a) & \vdots & \varphi_n^{(n-1)}(a)
        \end{bmatrix} + 
        \begin{bmatrix}
            N_{11} & \cdots & N_{1n}\\
            \vdots &  & \vdots\\
            N_{m1} & \cdots & N_{mn}
        \end{bmatrix}
        \begin{bmatrix}
            \varphi_1(b) & \cdots & \varphi_n(b)\\
            \varphi_1'(b) & \cdots & \varphi_n'(b)\\
            \vdots& & \vdots\\
            \varphi_1^{(n-1)}(b) & \vdots & \varphi_n^{(n-1)}(b)
        \end{bmatrix}\\
        &= \begin{bmatrix}
            \sum_{j=1}^nM_{1j}\varphi_1^{(j-1)}(a) & \cdots & \sum_{j=1}^nM_{1j}\varphi_n^{(j-1)}(a)\\
            \vdots & & \vdots\\
            \sum_{j=1}^nM_{mj}\varphi_1^{(j-1)}(a) & \cdots & \sum_{j=1}^nM_{mj}\varphi_n^{(j-1)}(a)
        \end{bmatrix} +
        \begin{bmatrix}
            \sum_{j=1}^nN_{1j}\varphi_1^{(j-1)}(b) & \cdots & \sum_{j=1}^nN_{1j}\varphi_n^{(j-1)}(b)\\
            \vdots & & \vdots\\
            \sum_{j=1}^nN_{mj}\varphi_1^{(j-1)}(b) & \cdots & \sum_{j=1}^nN_{mj}\varphi_n^{(j-1)}(b)
        \end{bmatrix}\\
        &= \begin{bmatrix}
            \sum_{j=1}^n(M_{1j}\varphi_1^{(j-1)}(a) + N_{1j}\varphi_1^{(j-1)}(b)) & \cdots & (\sum_{j=1}^nM_{1j}\varphi_n^{(j-1)}(a) + N_{1j}\varphi_n^{(j-1)}(b))\\
            \vdots & & \vdots\\
            \sum_{j=1}^n(M_{mj}\varphi_1^{(j-1)}(a) + N_{mj}\varphi_1^{(j-1)}(b)) & \cdots & (\sum_{j=1}^nM_{mj}\varphi_n^{(j-1)}(a) + N_{mj}\varphi_n^{(j-1)}(b))
        \end{bmatrix}\\
        &= \begin{bmatrix}
            U_1\varphi_1 & \cdots & U_1\varphi_n\\
            \vdots & & \vdots\\
            U_m\varphi_1 & \cdots & U_m\varphi_n
        \end{bmatrix}.
    \end{align*}
\end{rmk}
}

\begin{thm}\label{thm:exactly k l.i.d solutions iff rank n-k}
    The problem $\pi_m$ has exactly $k$ ($0\leq k\leq n$) linearly independent solutions if and only if $U\Phi$ has rank $n-k$, where $\Phi$ is any fundamental matrix associated with $Lx=0$.
\end{thm}
\begin{proof}
    A function $\varphi$ satisfies $Lx=0$ if and only if the corresponding vector $\vec{\varphi}=(\varphi, \varphi', \ldots, \varphi^{(n-1)})$ is of the form $\vec{\varphi}=\Phi \vec{c}$, where $\vec{c}=(c_1,\ldots, c_n)$ is a constant vector.

    \LIN{
    Indeed: Suppose $\varphi$ is a solution to $Lx=0$. Then by definition of fundamental set $\varphi_1,\ldots,\varphi_n$, $\varphi = c_1\varphi_1+\cdots+c_n\varphi_n$ for some $c_1,\ldots,c_n\in\C$. By linearity of derivatives, $\varphi^{(j)} = c_1\varphi_1^{(j)} + \cdots + c_n\varphi_n^{(j)}$. Thus,
    \begin{align*}
        \vec{\varphi} =
        \begin{bmatrix}
            \varphi\\
            \varphi'\\
            \vdots\\
            \varphi^{(n-1)}
        \end{bmatrix} &= 
        \begin{bmatrix}
            c_1\varphi_1 + \cdots + v_n\varphi_n\\
            c_1\varphi'_1 + \cdots + v_n\varphi'_n\\
            \vdots\\
            c_1\varphi^{(n-1)}_1 + \cdots + v_n\varphi^{(n-1)}_n\\
        \end{bmatrix}\\
        &= \begin{bmatrix}
            \varphi_1 & \cdots & \varphi_n\\
            \varphi_1' & \cdots & \varphi_n'\\
            \vdots &  & \vdots\\
            \varphi_1^{(n-1)} & \cdots & \varphi_n^{(n-1)}
        \end{bmatrix}
        \begin{bmatrix}
            c_1\\
            \vdots\\
            c_n
        \end{bmatrix} = \Phi \vec{c}.
    \end{align*}
    }
    Thus, $U\varphi=0$\unsure[inline]{This is the definition of $Ux$ in Remark \ref{rmk:writing Ux in M, N (M:N)}?} if and only if
    \[U(\Phi c) = (U\Phi)c = 0.\]
    Since $\dim(\text{solution space}) + \rank(\text{matrix}) = \text{$\#$ of unknown variables}$, the number of linearly independent vectors $\vec{c}$ satisfying $(U\Phi)c=0$ is $n-\rank(U\Phi)$. \LIN{Thus, the number of solutions $\varphi$ to $Lx=0$ is $n-\rank(U\Phi)$.}
    
    If $\Phi_1$ is any other fundamental matrix associated with $Lx=0$, then $\Phi_1=\Phi C$, where $C$ is a nonsingular constant matrix.\unsure{By change of basis?} Therefore
    \[\rank(U\Phi_1)=\rank(U\Phi).\]
\end{proof}

\begin{thm}\label{thm:exactly k l.i.d solutions implies exactly k+m-n l.i.d solutions}
    If $\pi_m$ has exactly $k$ linearly independent solutions, then $\pi_{2n-m}^+$ has exactly $k+m-n$ linearly independent solutions.
\end{thm}
\begin{proof}
    Let $\varphi_1,\ldots,\varphi_k$ be $k$ linearly independent solutions of $\pi_m$. Suppose $U_c$ where
    \[U_c x = M_c\xi(a) + N_c\xi(b)\]
    is a boundary form of rank $2n-m$ complementary to $U$. We show that the vectors $U_c \varphi_i$ ($i=1,\ldots,k$) are linearly independent. Suppose not, then for some constants $\alpha_1,\ldots,\alpha_k\in\C$ not all zero,
    \[\sum_{i=1}^k \alpha_i U_c\varphi_i=0,\]
    which implies
    \[U_c\left(\sum_{i=1}^k \alpha_i\varphi_i\right)=0.\]
    \LIN{But since each $\varphi_i$ is a solution to $\pi_m$, they each satisfy $Ux=0$. Thus,
    \[U\left(\sum_{i=1}^k \alpha_i\varphi_i\right)=0.\]
    Let $\bar{\varphi}=\sum_{i=1}^k \alpha_i\varphi_i$. Let $\bar{\xi}=(\bar{\varphi}, \bar{\varphi}', \ldots, \bar{\varphi}^{(n-1)})$. Then by Remark \ref{rmk:writing Ux in M, N (M:N)}, the above equations imply
    \begin{align*}
        M\bar{\xi}(a) + N\bar{\xi}(b) &= U\bar{\xi}=0\\
        M_c\bar{\xi}(a) + N_c\bar{\xi}(b) &= U_c\bar{\xi}=0.
    \end{align*}
    Or
    \begin{align*}
        \begin{bmatrix}
            M & N\\
            M_c & N_c
        \end{bmatrix}
        \begin{bmatrix}
            \bar{\xi}(a)\\
            \bar{\xi}(b)
        \end{bmatrix} =
        0_{2n\times 1}.
    \end{align*}
    But $\rank\begin{bmatrix}
        M & N\\
        M_c & N_c
    \end{bmatrix}=2n$, which implies it is nonsingular. Thus $\bar{\xi}(a) = \bar{\xi}(b)=0_{n\times 1}$. But since $\varphi_1,\ldots, \varphi_k$ are solutions to $Lx=0$, we have
    \begin{align*}
        L\bar{\varphi} &= L\left(\sum_{i=1}^k \alpha_i\varphi_i\right) = \sum_{i=1}^k \alpha_i L\varphi_i = 0.
    \end{align*}
    We show that this implies $\bar{\varphi}=0$. Indeed: If not, then $L$ maps a nonzero function to $0$, which means if two distinct functions $x_1$, $x_2$ are such that $x_1-x_2=\bar{\varphi}$, then $Lx_1-Lx_2=L(x_1-x_2)=0$, i.e., the pre-image of $0$ under $L$ is not unique. \unsure[inline]{This is how I interpreted ``uniqueness'' in the next line. But why is this a problem / where is the contradiction?}
    }
    Thus by uniqueness, $\bar{\varphi}(t)=0$ for $t\in [a,b]$. This contradicts the definition of $\bar{\varphi}$ as a nontrivial linear combination of $\varphi_1,\ldots,\varphi$ \LIN{(i.e., not all $\alpha_i$ are $0$)}. Hence
    \[\alpha_1=\alpha_2=\cdots=\alpha_k=0\]
    and $U_c\varphi_i$ are linearly independent.

    Let $\psi_1,\ldots,\psi_n$ be $n$ linearly independent solutions of $L^+x=0$. Suppose $\Psi$ is the corresponding fundamental matrix. Since $\varphi_i$, $\psi_j$ are solutions to $\pi_m$ and $L^+x=0$,\unsure[inline]{This is not the same as requiring $\psi_j$ to be solutions to $\pi_{2n-m}$, is it? Since there is an extra $U^+\psi_j=0$ to fulfill?} respectively, \LIN{by Proposition \ref{prop:(Lu,v)=(u,L^+v)}},\unsure[inline]{Proposition \ref{prop:(Lu,v)=(u,L^+v)} requires that $U\varphi_i=0$ and $U^+\psi_j=0$; are these conditions fulfilled?}
    \[(L\varphi_i, \psi_j) = (\varphi_i, L^+\psi_j).\]
    By Green's formula (\ref{eq:green's formula}),
    \[0 = (L\varphi_i, \psi_j) - (\varphi_i, L^+\psi_j) = [\varphi_i\psi_j](b) - [\varphi_i\psi_j](a)\]
    for $i=1,\ldots,k$, $j=1,\ldots,n$. By the boundary-form formula (\ref{eq:boundary form formula}),
    \[[\varphi_i\psi_j](b) - [\varphi_i\psi_j](a) = U_{\varphi_i}\cdot U_c^+\psi_j + U_{c\varphi_i}\cdot U^+\psi_j.\]
    \LIN{
    Since $\varphi_i$ are solutions to $\pi_m$, we have $U\varphi_i=0$ for $i=1,\ldots,k$.
    }
    Thus,
    \[U_{c\varphi_i}\cdot U^+\psi_j = 0.\]
    \unsure[inline]{Does this mean we don't know $U^+\psi_j=0$? If so, how could we use Proposition \ref{prop:(Lu,v)=(u,L^+v)} above?}
    By definition of $f\cdot g$\ref{defn:f cdot g}, $f\cdot g=g^*f$ for any column vectors $f, g$ of the same dimension, so
    \[(U^+\psi_j)^*U_{c\varphi_i}=0\quad\mbox{($i=1,\ldots,k$)}.\]
    We have shown before that $U_c\varphi_i$ are linearly independent. So the system $(U^+\psi_j)^*v=0$ has (at least) the $k$ linearly independent $(2n-m\times 1)$ vectors $U_c\varphi_1,\ldots, U_c\varphi_k$ as solutions. Therefore,
    \[\rank(U^+\Psi) = \rank(U^+\Psi)^*\leq (2n-m)-k.\]

    Suppose $\rank(U^+\psi)=r<(2n-m)-k$. Then by similar reasoning it can be shown that, if $\Phi$ is any fundamental matrix associated with $Lx=0$, $\rank(U\phi)\leq m - (n-r)<n-k$. By Theorem \ref{thm:exactly k l.i.d solutions iff rank n-k}, this contradicts with the assumption that $\pi_m$ has exactly $k$ linearly independent solutions. Thus, we must have
    \[\rank(U^+\Psi)=2n-m-k.\]
    By Theorem \ref{thm:exactly k l.i.d solutions iff rank n-k}, there exist exactly $k+m-n$ linearly independent solutions of $\pi_{2n-m}^+$.
\end{proof}

\begin{cor}
    $\pi_n$ and $\pi^+_n$ have the same number of independent solutions. %\DAS{typo}
\end{cor}
\LIN{
\begin{proof}
    Apply Theorem \ref{thm:exactly k l.i.d solutions implies exactly k+m-n l.i.d solutions} on $m=n$.
\end{proof}
}
\DAS{Coddington \& Levinson go to a lot of trouble here to count the dimension of the solution space of problems in general. In practice, we will be working in problems in which the rank of $(M:N)$ is equal to the order of $L$, so this can be greatly simplified. If you like, one later direction of the capstone would be to understand how this kind of stuff works in some more general setting, but my feeling is that is not the most need. Maybe don't spend too much time on this for now.}

\subsubsection{Nonhomogeneous Boundary-value Problems and Green's Function}
\begin{defn}
    A \href[page=306]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{nonhomogeneous boundary-value problem}} associated with $\pi_m$ is a problem of the form
    \begin{equation}\label{eq:nonhomogeneous boundary-value problem}
        Lx=f\quad Ux=\gamma
    \end{equation}
    on $t\in[a,b]$, where $f$ is a complex-valued continuous function on $[a,b]$ and $\gamma$ is a complex constant vector such that either $f$ is not the zero function or $\gamma\neq 0$.
\end{defn}
\begin{rmk}
    If $\varphi$ and $\bar{\varphi}$ are two solutions of \ref{eq:nonhomogeneous boundary-value problem}, their difference $\varphi-\bar{\varphi}$ is a solution of $\pi_m$. Hence, if $\pi_m$ has $k$ linearly independent solutions $\varphi_1,\ldots,\varphi_k$, then $\varphi=\bar{\varphi}+\sum_{i=1}^k c_i\varphi_i$ for some constants $c_i\in\C$ \LIN{(since $\varphi_1,\ldots,\varphi_k$ are a basis for the solution space of $\pi_m$)}.
\end{rmk}

\begin{prop}\label{prop:Ax=b has a solution iff condition}
    Let $A$ be a matrix and $b$ a vector. $Ax=b$ has a solution if and only if $b\cdot u=u^*b=0$ for every solution $u$ of $A^*x=0$.
\end{prop}

\begin{thm}\label{thm:nonhomogeneous problem has solution iff condition}
    The nonhomogeneous problem \ref{eq:nonhomogeneous boundary-value problem} has a solution if and only if
    \begin{equation}\label{eq:nonhomogeneous problem has solution iff condition}
        (f,\psi)=\gamma\cdot U_c^+\psi
    \end{equation}
    holds for every solution $\psi$ of the adjoint homogeneous problem $\pi_{2n-m}^+$.
\end{thm}
\begin{rmk}
    Since $(f,\psi)$ is an inner product, $\gamma=0$ implies $f$ is orthogonal to all solutions $\psi$ of $\pi_{2n-m}^+$.
\end{rmk}
\begin{proof}
    Let $\varphi$ be a solution of \ref{eq:nonhomogeneous boundary-value problem}. Let $\psi$ be a solution of the adjoint homogeneous problem $\pi_{2n-m}^+$. Then by Green's formula (\ref{eq:green's formula}) and the boundary-form formula (\ref{eq:boundary form formula}), 
    \begin{align*}
        (L\varphi, \psi) - (\varphi, L^+\psi) &= U\varphi\cdot U_c^+\psi + U_c\varphi\cdot U^+\psi.
    \end{align*}
    Since $L\varphi=f$, $U\varphi=\gamma$, $L^+\psi=0$, and $U^+\psi=0$, the above implies that
    \begin{align*}
        (f,\psi) - (\varphi, 0) = \gamma\cdot U_c^+\psi + 0,
    \end{align*}
    or
    \[(f,\psi)=\gamma\cdot U_c^+\psi.\]
    Now suppose \ref{eq:nonhomogeneous problem has solution iff condition} holds for all solutions $\psi$ of $\pi_{2n-m}^+$. Let $\varphi_1,\ldots,\varphi_n$ be a fundamental set for $Lx=0$. Let $\bar{\varphi}$ be a solution of $Lx=f$. Then every solution $\varphi$ of $Lx=f$ is of the form
    \[\varphi = \bar{\varphi} + \sum_{i=1}^n c_i\varphi_i\]
    for some constants $c_i\in\C$. Applying $U$ to both sides, we have that \ref{eq:nonhomogeneous boundary-value problem} has a solution if and only if there exist $c_i$ such that
    \[U\varphi = U\bar{\varphi} + \sum_{i=1}^n c_i U\varphi_i\]
    is equal to $\gamma$. Equivalently,
    \begin{equation}\label{eq:(U Phi)c=}
        (U\Phi)c=\gamma-U\bar{\varphi}
    \end{equation}
    where $\Phi$ is the fundamental matrix corresponding to $\varphi_1,\ldots,\varphi_n$, and $c$ a constant vector.
    \LIN{
    Note that by Remark \ref{rmk:U Phi},
    \begin{align*}
        (U\Phi)c &= \begin{bmatrix}
            U_1\varphi_1 & \cdots & U_1\varphi_n\\
            \vdots & & \vdots\\
            U_m\varphi_1 & \cdots & U_m\varphi_n\\
        \end{bmatrix} \begin{bmatrix}
            c_1\\
            \vdots\\
            c_n
        \end{bmatrix}\\
        &= \begin{bmatrix}
            \sum_{i=1}^n c_i U_1\varphi_i\\
            \vdots\\
            \sum_{i=1}^n c_i U_m\varphi_i
        \end{bmatrix}\\
        &= \sum_{i=1}^n\begin{bmatrix}
            c_i U_1\varphi_i\\
            \vdots\\
            c_i U_m\varphi_i
        \end{bmatrix}\\
        &= \sum_{i=1}^n c_i U\varphi_i
    \end{align*}
    }
    By Proposition \ref{prop:Ax=b has a solution iff condition}, the above has a solution $c$ if and only if $\gamma-U\bar{\varphi}$ is orthogonal to every solution $u$ of
    \begin{equation}\label{eq:U Phi * u = 0}
    (U\Phi)^* u=0,
    \end{equation}
    that is,
    \begin{equation}\label{eq:orthogonal with u}
        (\gamma-U\bar{\varphi})\cdot u = 0
    \end{equation}
    Suppose $\pi^+_{2n-m}$ have exactly $k^+$ linearly independent solutions $\psi_1,\ldots,\psi_{k^+}$. By the same argument in Theorem \ref{thm:exactly k l.i.d solutions implies exactly k+m-n l.i.d solutions}, the $k^+$ vectors $U_c^+\psi_1,\ldots, U_c^+\psi_{k^+}$ are linearly independent $m$-dimensional vectors which are solutions to \ref{eq:U Phi * u = 0}. Let $k$ be the number of linearly independent solutions of $\pi_m$. Then by Theorem \ref{thm:exactly k l.i.d solutions iff rank n-k}, $\rank(U\Phi)=n-k$. Thus, the number of linearly independent solutions of \ref{eq:U Phi * u = 0} is $m-\rank(U\Phi^*) = m-\rank(U\Phi) = m - (n-k)$. But by Theorem \ref{thm:exactly k l.i.d solutions implies exactly k+m-n l.i.d solutions}, $k^+ = m-n+k$. Thus, \LIN{since $U_c^+\varphi_i$ are basis for the solution space of \ref{eq:U Phi * u = 0}}, \ref{eq:orthogonal with u} holds for every $u$ satisfying \ref{eq:U Phi * u = 0} if and only if
    \begin{equation}\label{eq:orthogonal with U_c^+varphi_i}
        (\gamma-U\bar{\varphi})\cdot U_c^+\psi_i=0\quad(i=1,\ldots,k^+)
    \end{equation}
    By Green's formula (\ref{eq:green's formula}) and the boundary-form formula (\ref{eq:boundary form formula}),
    \LIN{
    \begin{equation}
        \begin{split}
        (L\bar{\varphi},\psi_i) - (\bar{\varphi}, L^+\psi_i) &= U_{\bar{\varphi}}\cdot U_c^+\psi_i + U_c\bar{\varphi}\cdot U^+\psi_i.
        \end{split}
    \end{equation}
    But $\psi_i$ is a solution to $\pi^+_{2n-m}$, so $L^+\psi_i=0$ and $U^+\psi_i=0$. Thus, the above becomes
    \begin{equation}\label{eq:(f,psi_i)}
        (f,\psi_i)=(L\bar{\varphi},\psi_i) = (L\bar{\varphi},\psi_i) - 0 =  U_{\bar{\varphi}}\cdot U_c^+\psi_i + 0 =  U_{\bar{\varphi}}\cdot U_c^+\psi_i.
    \end{equation}
    But by the hypothesis, \ref{eq:nonhomogeneous problem has solution iff condition} holds. Thus, 
    \[
        U_{\bar{\varphi}}\cdot U_c^+\psi_i \stackrel{\text{\ref{eq:(f,psi_i)}}}{=} (f,\psi_i) \stackrel{\text{\ref{eq:nonhomogeneous problem has solution iff condition}}}{=} \gamma\cdot U_c^+\psi_i.
    \]
    Hence,
    \[(\gamma-U\bar{\varphi})\cdot U_c^+\psi_i=0.\]
    So \ref{eq:orthogonal with U_c^+varphi_i} is satisfied. Reversing the direction of the argument from \ref{eq:orthogonal with U_c^+varphi_i}, we have that \ref{eq:orthogonal with u} holds. This implies that \ref{eq:(U Phi)c=} has a solution $c$, which then implies that \ref{eq:nonhomogeneous boundary-value problem} has a solution.
    }
\end{proof}

\begin{cor}
    \ref{eq:nonhomogeneous boundary-value problem} has a unique solution if $m=n$ and the only solution of $\pi_n$ is the trivial one.
\end{cor}
\begin{proof}
    \LIN{
    Suppose $m=n$ and $\pi_n$ only has the trivial solution (note that $\pi_n$ is the homogeneous problem). 
    % By Theorem \ref{thm:exactly k l.i.d solutions implies exactly k+m-n l.i.d solutions}, $\pi_n^+$ has only $1+n-n=1$ solution. But we already know that the trivial solution is one solution of $\pi_n^+$. So $\pi_n^+$ has only the trivial solution $\psi=0$. 
    Let $\varphi$, $\bar{\varphi}$ be two solutions of \ref{eq:nonhomogeneous boundary-value problem}. Then $\varphi-\bar{\varphi}$ is a solution of $\pi_n$. By the hypothesis, $\varphi-\bar{\varphi}=0$. Thus, \ref{eq:nonhomogeneous boundary-value problem} has a unique solution $\varphi$.
    }
    % \unsure[inline]{This is different from \href[page=307]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{the proof in the book}. Actually in the corollary statement, is the sentence after ``and'' part of the conclusion or condition?}
	% \DAS{It is part of the condition. Your proof is good.}
\end{proof}

\DAS{I suggest we come back to Green's functions, and inhomogeneous BVP if and only if we decide to go in that direction. For now, the below is great progress.}

\LIN{
\begin{defn}
    The \textbf{Dirac's delta function} can be loosely thought of as a function on $\R$ which is zero everywhere except at the origin, where it is infinite. It is given by
    \[\delta(x)=
    \begin{cases}
        +\infty\quad&\mbox{$x=0$}\\
        0\quad&\mbox{$x\neq 0$}.
    \end{cases}    
    \]
    It is also constrained to satisfy the identity
    \[\int_{-\infty}^\infty\delta(x)\,dx=1.\]
\end{defn}

\begin{defn}\footnote{\url{http://mathworld.wolfram.com/GreensFunction.html}}
    Given a linear differential operator $L=L(x)$, a \textbf{Green's function} $G=G(x,s)$ at the point $s\in\Omega\in\R^n$ corresponding to $L$ is any solution of
    \begin{equation}\label{eq:LG(x,s)=delta(x-s)}
        LG(x,s)=\delta(x-s)
    \end{equation}
    where $\delta$ denotes the Dirac's delta function.
\end{defn}

\begin{rmk}
    A Green's function is an \textbf{integral kernel} that can be used to solve differential equations, such as ordinary differential equations with initial or boundary value conditions. Recall the Fourier kernel $e^{i\lambda x}$ in the Fourier transform
    \[\mathcal{F}[f]:=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty f(x)e^{i\lambda x}\,dx.\]

    To see the motivation for defining Green's function, we note that multiplying both sides of \ref{eq:LG(x,s)=delta(x-s)} by a function $f(s)$ and integrating with respect to $s$ gives
    \[\int LG(x,s)f(s)\,ds = \int \delta(x-s)f(s)\,ds.\]
    The right-hand side reduces to $f(x)$ due to properties of $\delta$, and because $L$ is a linear operator acting only on $x$ and not on $s$, the left-hand side can be rewritten as 
    \[L\left(\int G(x,s)f(s)\,ds\right).\]
    This reduction is particularly useful when solving for $u=u(x)$ in differential equations of the form
    \[Lu(x)=f(x),\]
    since we have
    \[Lu(x) = L\left(\int G(x,s)f(s)\,ds\right),\]
    which implies
    \begin{equation}\label{eq:Green's function as DE solution}
        u(x)=\int G(x,s)f(s)\,ds.
    \end{equation}
\end{rmk}
}

Suppose $m=n$. By Theorem \ref{thm:exactly k l.i.d solutions implies exactly k+m-n l.i.d solutions}, $\pi_n$ and $\pi^+_n$ have the same number $k$ of linearly independent solutions. If $k=0$, then $\pi_n$  has only the trivial solution. In this case, it is possible to solve the nonhomogeneous problem \ref{eq:nonhomogeneous boundary-value problem} explicitly in terms of the Green's function:

\LIN{
\begin{prop}\label{prop:green's function as solution}
    The nonhomogeneous problem
    \[Lx=f\quad Ux=0\]
    has a unique solution given by $x(t)=\int_a^b f(s)G(s,t)\,ds$ where $G(s,t)$ is a Green's function satisfying some properties.
\end{prop}

\begin{defn}\label{defn:eigenvalue problem}
    The problem
    \[\pi:\quad Lx=\ell x\quad Ux=0\]
    is an \href[page=201]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{\textbf{eigenvalue problem}}. If $\ell$ is such that $\pi$ has a nontrivial solution, then $\ell$ is an \textbf{eigenvalue} of $\pi$, and the nontrivial solutions of $\pi$ are the \textbf{eigenfunctions} of $\pi$.
\end{defn}
\begin{rmk}
    Recall from linear algebra that if $T$ is a linear transformation from a vector space $V$ over a field $\F$ into itself and $v\in V$, $v\neq \vec{0}$, then $v$ is an eigenvector of $T$ if $T(v)=\lambda v$ for some $\lambda\in\F$, and $\lambda$ the eigenvalue associated with $v$. 

    Here, if $L$ is a linear differential operator, and $u$ is not the zero function, then $u$ is an eigenfunction of $\pi$ if $Lu=\ell u$ for some $\ell$, and $\ell$ the eigenvalue associated with $u$.
\end{rmk}

We introduce \href[page=204]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{the following results}.

\begin{prop}\label{prop:existence of G}
    Let $\pi$ denote the problem
    \[Lx-\ell x = 0 \quad Ux=0.\]
    If $\pi$ has no (nontrivial) solution for at least one value of $\ell$, then there exists unique $G=G(t,\tau,\ell)$ defined for $(t,\tau)$ on the square $a\leq t,\tau\leq b$ and for all complex $\ell$ except the eigenvalues of $\pi$ and having the following properties:
    \begin{itemize}
        \item[(i)] $\frac{\partial ^k G}{\partial t^k}$ ($k=0,1,\ldots,n-2$) exist and are continuous in $(t,\tau,\ell)$ for $(t,\tau)$ on the square $a\leq t$, $\tau\leq b$ and $\ell$ not at an eigenvalue of $\pi$. Moreover, $\frac{\partial ^k G}{\partial t^k}$ for $k=n-1, n$ are continuous in $(t,\tau,\ell)$ for $(t,\tau)$ on the triangles $a\leq t\leq \tau\leq b$ and $a\leq \tau\leq t\leq b$ and $\ell$ not an eigenvalue of $\pi$.
        \item[(ii)]
        \[\frac{\partial^{n-1}G}{\partial t^{n-1}}(r+0, r, \ell) - \frac{\partial^{n-1}G}{\partial t^{n-1}}(r-0, r, \ell) = \frac{1}{p_0(\tau)}.\]
        \item[(iii)] As a function of $t$, $G$ satisfies $Lx=\ell x$ if $t\neq \tau$.
        \item[(iv)] As a function of $t$, $G$ satisfies the boundary conditions $Ux=0$ for $a\leq \tau\leq b$.
    \end{itemize}

    If for one value of $\ell$ the homogeneous problem $\pi$ has no solution, then it would have solutions only for a set of $\ell$ which are the zeroes of an entire function.
\end{prop}
}
Here, $\ell=0$ and it is assumed that $\pi_n$ has only the trivial solution. Denote $G(t,\tau,0)$ as $G(t,\tau)$. By \eqref{eq:Green's function as DE solution}, the unique solution of \ref{eq:nonhomogeneous boundary-value problem} with $\gamma=0$ is given by
\[\mathcal{G}f(t):=\int_a^b G(t,\tau)f(\tau)\,d\tau.\]
If $\pi_n$ has only the trivial solution, then by Theorem \ref{thm:exactly k l.i.d solutions implies exactly k+m-n l.i.d solutions}, $\pi_n^+$ has only the trivial solution (with $k=1$ and $m=n$). By Proposition \ref{prop:existence of G}, the Green's function $G^+$ for $\pi_n^+$ exists and is unique.

\begin{thm}\label{thm:G^+(t,tau) = bar G(tau, t)}
    If $\pi_n$ has only the trivial solution, Green's function $G^+$ for $\pi_n^+$ is given by
    \begin{equation}\label{eq:G^+(t,tau)=bar G(tau, t)}
        G^+(t,\tau)=\bar{G}(\tau,t).
    \end{equation}
\end{thm}
\begin{proof}
    Let $a<\tau_1<\tau_2<b$. Consider the functions $G_1$ and $G_2^+$ given by $G_1(t)=G(t,\tau_1)$, $G_2^+(t)=G^+(t,\tau_2)$. Then applying Green's formula \ref{eq:green's formula} to the intervals $[a, \tau_1-0]$, $[\tau_1+0, \tau_2-0]$, $[\tau_2+0, b]$, we have
    \begin{equation}\label{eq:green's formula on G_1G_2^+}
        [G_1G_2^+](\tau_1-0) - [G_1G_2^+](a) + [G_1G_2^+](\tau_2-0) - [G_1G_2^+](\tau_1+0) + [G_1G_2^+](b) - [G_1G_2^+](\tau_2+0)=0
    \end{equation}
    \unsure[inline]{Why is the equation $0$?}
    By the boundary-form formula \ref{eq:boundary form formula},
    \begin{equation}
        [G_1G_2^+](b) - [G_1G_2^+](a)=0.
    \end{equation}
    \unsure[inline]{Doesn't this follow from \ref{eq:green's formula on G_1G_2^+}?}
    From the form of $[xy](t)$ \ref{eq:[uv](t) defn} it follows that the only terms of interest in \ref{eq:green's formula on G_1G_2^+} are those involving the $(n-1)$st derivatives\unsure{Why?}, and these are
    \begin{equation}\label{eq:n-1st derivatives in [xy](t)}
        p_0(t)[(-1)^{n-1}x(t)\bar{y}^{(n-1)}(t) + x^{(n-1)}(t)\bar{y}(t)].
    \end{equation}
    Now by Proposition \ref{prop:existence of G}(ii), $G$ satisfies
    \begin{equation}
        \frac{\partial^{n-1}G}{\partial t^{n-1}}(r+0, r, \ell) - \frac{\partial^{n-1}G}{\partial t^{n-1}}(r-0, r, \ell) = \frac{1}{p_0(\tau)}.
    \end{equation}
    Similarly for $G^+$,
    \begin{equation}
        \frac{\partial^{n-1}G^+}{\partial t^{n-1}}(r+0, r, \ell) - \frac{\partial^{n-1}G^+}{\partial t^{n-1}}(r-0, r, \ell) = \frac{1}{(-1)^n \bar{p}_0(\tau)}.
    \end{equation}
    Thus, $\bar{G}^+(\tau_1,\tau_2) - G(\tau_2,\tau_1)=0$.\unsure{How?}
\end{proof}

\begin{rmk}
    To consider $G(t,\tau,\ell)$, the differential operation $(L-l)$ is considered instead of $L$. Let $L_1=L-\ell$. Consider the problem
    \begin{equation}\label{eq:(L-l)x=0}
        L_1x=0\quad Ux=0
    \end{equation}
    The adjoint problem is given in $L_1^+=L^+-\bar{\ell}$ and $U^+$. Applying Theorem \ref{thm:G^+(t,tau) = bar G(tau, t)} to \ref{eq:(L-l)x=0},\unsure[inline]{We assume $\pi_n$ only has the trivial solution?} we have
    \[G^+(t,\tau,\ell) = \bar{G}(\tau,t,\bar{\ell}).\]
    For the self-adjoint problem where $L^+=L$ and $U$ equivalent to $U^+$, it follows that 
    \[G(t,\tau,\ell) = \bar{G}(\tau,t,\ell).\]
\end{rmk}

\newpage
\subsection{Chapter 12: Non-self-adjoint boundary-value problems}
\subsubsection{Introduction}
Let $L$ be an $n$th-order ordinary differential operator which is formally self-adjoint. Consider a self-adjoint boundary-value problem
\[\pi: Lx=\ell x\quad Ux=0\]
on $[a,b]$ {(\color{blue}where $L=L^+$ and $U=U^+$)}. Then there exists a complete orthonormal set of eigenfunctions $\{\chi_k\}$ and a Green's function $G(t,\tau,\ell)$ for the equation $Lx=\ell x$ where $\ell$ is not one of the eigenvalues of $\pi$.

\begin{thm}{\href[page=209]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{(Eigenfunction expansion theorem)}}\label{thm:eigenfunction expansion}
   Let $f\in C^n$ on $[a,b]$ and satisfy the boundary condition $Uf=0$. Then on $[a,b]$,
    \begin{equation}
        f=\sum_{k=0}^\infty (f,\chi_k)\chi_k
    \end{equation}
    where $(f,\chi_k)=\int_a^b f(t)\bar{\chi}_k(t)\,dt$ and the series converges uniformly on $[a,b]$.
\end{thm}

\newpage
\section{Finding ways to explicitly construct (a non-unique) $U^+$ given $U$}
We focus on the case where $m=n$. That is, $M$ and $N$ are $n\times n$ square matrices.

% Recall from Theorem \ref{thm:condition iff adjoint} that, given a boundary condition $Ux=0$ where
% \[Ux = M\xi(a) + N\xi(b)\] with $\rank(M:N)=n$, a boundary condition $U^+x=0$ where
% \[U^+x = P^*\xi(a) + N^*\xi(b)\] with $\rank(P^*:Q^*)=n$ is adjoint to $Ux=0$ if and only if
% \[MB^{-1}(a)P = NB^{-1}(b)Q\]
% where $B(t)$ is the $n\times n$ matrix associated with the form $[xy](t)$ (\ref{eq:[uv](t) in B matrix}). Note that $M, N, P, Q$ are not necessarily nonsingular (there is no restriction on their rank, and if they are not full rank, since they are square matrices, they are singular).

\subsection{A simple example of the hard way}
\href[page=11]{./literature/Constructing adjoint boundary conditions/Giles-Pierce2000_Article_AnIntroductionToTheAdjointAppr.pdf}{For $L$ with lower orders, the adjoint boundary condition can be (feasibly) found using integration by parts.}

Consider $Lx=x^{(1)} - \epsilon x^{(2)}$ on $[0,1]$ with boundary conditions $x(0)=x(1)=0$. Then
\[Lx = (-\epsilon) x^{(2)} + 1x^{(1)} + 0x^{(0)}\]
with $p_0 = -\epsilon$, $p_1 = 1$, and $p_2 = 0$. And
\[Ux=0\]
where
\begin{align*}
    Ux &= \begin{bmatrix} U_1x\\ U_2x \end{bmatrix}\\
    &= \begin{bmatrix} x(0)\\ x(1)\end{bmatrix} \\
        &= \begin{bmatrix}
    (1\cdot x^{(0)}(0) + 0\cdot x^{(0)}(1)) + (0\cdot x^{(1)}(0) + 0\cdot x^{(1)}(1))\\
    (0\cdot x^{(0)}(0) + 1\cdot x^{(0)}(1)) + (0\cdot x^{(1)}(0) + 0\cdot x^{(1)}(1)) \end{bmatrix}\\
    &=
\begin{bmatrix}
    1 & 0\\
    0 & 0
\end{bmatrix}
\begin{bmatrix}
    x(0)\\
    x'(0)
\end{bmatrix} +
\begin{bmatrix}
    0 & 0\\
    1 & 0
\end{bmatrix}
\begin{bmatrix}
    x(1)\\
    x'(1)
\end{bmatrix} =
M\xi(0) + N\xi(1)
= \vec{0}.
\end{align*}
Using integration by parts,
\begin{align*}
    (y, Lx) &= \int_0^1 y (\overline{Lx})\,dt\\
    &= \int_0^1 y (\frac{dx}{dt} - \epsilon \frac{d^2 x}{dt^2})\,dt\\
    &= \int_0^1 y\frac{dx}{dt}\,dt - \epsilon\int_0^1 y\frac{d^2 x}{dt^2}\,dt\\
    &= \left(\left[yx\right]_0^1 - \int_0^1 x\frac{dy}{dt}\,dt\right) - \epsilon\left(\left[y\frac{dx}{dt}\right]_0^1 - \int_0^1 \frac{dx}{dt}\frac{dy}{dt}\,dt\right)\\
    &= \left(\left[yx\right]_0^1 - \int_0^1 x\frac{dy}{dt}\,dt\right) - \epsilon\left(\left[y\frac{dx}{dt}\right]_0^1 - \left(\left[x\frac{dy}{dt}\right]_0^1 - \int_0^1 x\frac{d^2 y}{dt^2}\,dt\right)\right)\\
    &= \int_0^1 x\left(-\frac{dy}{dt} - \epsilon\frac{d^2 y}{dt^2}\right)\,dt + \left[yx - \epsilon y\frac{dx}{dt} + \epsilon x\frac{dy}{dt}\right]_0^1\\
    &= \int_0^1 x\left(-\frac{dy}{dt} - \epsilon\frac{d^2 y}{dt^2}\right)\,dt + \left[-\epsilon y\frac{dx}{dt}\right]_0^1.
\end{align*}
For the above to equal
\[(L^+y, x),\]
we ought to define
\[
    L^+y := -y' - \epsilon y''  
\]
with $y(0)=y(1)=0$ for the boundary terms to vanish. Thus, the adjoint boundary condition is
\[U^+y=0\] with
\[U^+y = P^*\eta(0) + Q^*\eta(1)\] where 
\[P^* = \begin{bmatrix} 1 & 0\\0 & 0 \end{bmatrix};\quad 
Q^* = \begin{bmatrix} 0 & 0\\ 0 & 1\end{bmatrix}.\]
So we have
\[M = \begin{bmatrix} 1 & 0\\0 & 0 \end{bmatrix} \quad 
N = \begin{bmatrix} 0 & 0\\ 0 & 1\end{bmatrix} \quad
P = \begin{bmatrix} 1 & 0\\0 & 0 \end{bmatrix} \quad 
Q = \begin{bmatrix} 0 & 0\\ 0 & 1\end{bmatrix}.\]
The matrix $B(t)$ in this case would be the $2\times 2$ matrix associated with $[xy](t)$. Recall that 
\[Lx = -\epsilon x'' + x'= p_0x^{(2)} + p_1x^{(1)}.\]
So $p_0=-\epsilon$, $p_1=1$ are constant functions.
By Theorem \ref{thm:green's formula}, $[xy](t)$ is given by
\begin{align*}
    [xy](t) &= \sum_{m=1}^n\sum_{j+k=m-1}(-1)^j x^{(k)}(t)(p_{n-m}\bar{y})^{(j)}(t)\\
    &= \sum_{m=1}^2\sum_{j+k=m-1}(-1)^j x^{(k)}(t)(p_{2-m}\bar{y})^{(j)}(t)\\
    &= (-1)^0x(t)(p_1\bar{y})(t) + (-1)^0x^{(1)}(t)(p_0\bar{y})(t) + (-1)^1x(t)(p_0\bar{y})^{(1)}(t)\\
    &= x(t)p_1(t)\bar{y}(t) + x'(t)p_0(t)\bar{y}(t) - x(t)(p_0'(t)\bar{y}(t) + p_0(t)\bar{y}'(t))\\
    &= x(t)\bar{y}(t) + x'(t)(-\epsilon)\bar{y}(t) - x(t)(-\epsilon)\bar{y}'(t)\\
    &= x(t)\bar{y}(t) - \epsilon x'(t)\bar{y}(t) + \epsilon x(t)\bar{y}'(t)\\
    &= B_{11}(t)x(t)\bar{y}(t) + B_{12}(t)x'(t)\bar{y}(t) + B_{21}(t)x(t)\bar{y}'(t) + B_{22}(t)x'(t)\bar{y}'(t).
\end{align*}
Thus,
\[
B(t) = \begin{bmatrix}
1 & -\epsilon\\
\epsilon & 0
\end{bmatrix}.
\]
Since $p_0$, $p_1$ are constant functions, $B(0)=B(1)$.
We verify that
\begin{align*}
    MB^{-1}(0)P &= \begin{bmatrix} 1 & 0\\0 & 0 \end{bmatrix} 
    \left(\frac{1}{1+\epsilon^2}\begin{bmatrix} 0 & \epsilon\\-\epsilon & 1 \end{bmatrix} \right)
    \begin{bmatrix} 1 & 0\\0 & 0 \end{bmatrix}\\
        &= N B^{-1}(1) Q.
\end{align*}
So $U^+x=0$ is indeed adjoint to $Ux=0$ by Theorem \ref{thm:condition iff adjoint}. 

\subsection{Construction from existence theorem}
Given 
\[Lx = 0 \quad Ux=0\]
on $[a,b]$ where $\rank(U)=m=n$, we use Theorem \ref{thm:boundary form formula} to construct $U^+$ as follows.

\subsubsection{Working with $U_i$ in a vector space}
% \LIN{This section is more like my conceptual explorations. I think the next section is far more workable in terms of implementation.}

Recall from Linear algebra that \href[page=117]{./textbooks/Linear Algebra Done Right 3rd ed.pdf}{the dual space of a vector space $V$ is the vector space of all linear functionals on $V$, i.e., $\mathcal{L}(V,\mathcal{F})$}. Consider the vector space $C^{n-1}_{[a,b]}$. The dual space of $C^{n-1}_{[a,b]}$ is very large, but the subspace spanned by the boundary forms (recall that boundary forms map functions in $C^{n-1}_{[a,b]}$ to the field $\C$) is finite dimensional and has a basis of the $2n$ boundary forms $x \mapsto x^{(j)}(a)$ and $x \mapsto x^{(j)}(b)$ for $j=0,1,\cdots,n-1$ and $x\in C^{n-1}_{[a,b]}$ (it is clear that all boundary forms are linear combinations of them).

Let $\mathcal{V}$ denote the subspace with dimension $2n$ spanned by the boundary forms.
% \unsure[inline]{Is this how $\mathcal{V}$ should be characterized? I.e., $\mathcal{V}$ is a space of operators that map functions $x\in C^{n-1}_{[a,b]}$ to $\R$? Should I prove that it's a vector space? Or should I work with the matrices $M$, $N$ instead (see the next section)?}  
Then $U=(U_1,\ldots,U_n)$ is a list of $n$ linearly independent vectors in $\mathcal{V}$. 

\begin{algorithm}[H]
    \caption{Checking linear independence.}\label{algo:checking linear independence}
    \KwData{A list of boundary operators $U_1,\ldots, U_n$}
    \KwResult{Whether they are linearly independent}
    \unsure[inline]{The criterion for linear independence of the $U_i$ is $\sum_{i=1}^n c_i U_i x=0$ implies $c_i=0$ for all $x\in C^{n-1}_{[a,b]}$. How should I check this (since $C^{n-1}_{[a,b]}$ is infinite)? In general, it seems that I have to deal with criteria like this in the following algorithms working with $U_i$ and their spans.}
\end{algorithm}

Find a basis for $\mathcal{V}$ by building a maximally linearly independent set adding one vector at a time until the set spans $V$, \href[page=62]{./textbooks/Linear Algebra Done Right 3rd ed.pdf}{or equivalently, contains $2n$ linearly independent vectors}. 

\begin{algorithm}[H]
    \caption{Finding a basis.}\label{algo:finding a basis}
    \KwData{An arbitrary nonzero $V_1\in\mathcal{V}$}
    \KwResult{A list $V_1,\ldots,V_{2n}$ of linearly independent vectors in $\mathcal{V}$}
    \Begin{
        $l \longleftarrow [V_1]$\;
        \While{$\len(l)<2n$} {
            Pick any $V\in\mathcal{V} - \Span(l)$\;
            $l \longleftarrow l + V$
        }
        \Return{$l$}
    }
\end{algorithm}

By \href[page=58]{./textbooks/Linear Algebra Done Right 3rd ed.pdf}{a result from linear algebra}, using this basis, $U_1,\ldots,U_n$ can be extended to $U_1,\ldots,U_{2n}$, a basis of $\mathcal{V}$. Thus, we have found $U_c=(U_{n+1},\ldots,U_{2n})$.

\begin{algorithm}[H]
    \caption{Extending to a basis.}\label{algo:extending to a basis}
    \KwData{$U_1,\ldots,U_n$ and basis $V_1,\ldots,V_{2n}$}
    \KwResult{A basis $U_1,\ldots,U_{2n}$ of $\mathcal{V}$}
    \Begin{
        $l \longleftarrow [U_1,\ldots,U_n, V_1,\ldots, V_{2n}]$\;
        $i \longleftarrow 1$\;
        \While{$\len(l)> 2n$}{
            \If{$i=1$}{
                \If{$V_i\in \Span(U_1,\ldots,U_n)$}{
                    $l \longleftarrow [U_1,\ldots,U_n, V_{i+1},\ldots,V_{2n}]$
                }
            }
            \Else{
                \If{$V_i\in\Span(U_1,\ldots,U_n,\ldots,V_{i-1})$}{
                    $l \longleftarrow [U_1,\ldots,U_n, V_{i+1},\ldots,V_{2n}]$
                    }
            }
        }
        \Return{$l$}
    }
\end{algorithm}

Let $M, N, \tilde{M}, \tilde{N}$ be such that
\[Ux = M\xi(a) + N\xi(b);\quad U_cx = \tilde{M}\xi(a) + \tilde{N}\xi(b).\]
Then we have found
\[H = \begin{bmatrix} M & N\\\tilde{M} & \tilde{N}\end{bmatrix}.\]
Let $\hat{B}$ be as in \ref{eq:[uv](t) in B matrix} which depends on $L$. By Proposition \ref{prop:unique nonsingular G in semibilinear form}, $J=(\hat{B}H^{-1})^*$ is a nonsingular $2n\times 2n$ matrix such that $\mathcal{S}(f,g) = \hat{B}f\cdot g = Hf\cdot Jg$. Take $U^+y$ to be the last $n$ rows of $Jg$. Then we have found $U^+$.

\subsubsection{Working with matrices $M$, $N$}
For $U=(U_1,\ldots, U_n)$, write
\[Ux = M\xi(a) + N\xi(b).\]
Then \href[page = 298]{./textbooks/[Earl_A_Coddington_Norman_Levinson]_Theory_of_Ordinary_Differential_Equations.pdf}{$U_1,\ldots, U_n$ are linearly independent if and only if $\rank(M:N)=n$}. Thus, Algorithm \ref{algo:checking linear independence} can be replaced by the following.

\begin{algorithm}[H]
    \caption{Checking linear independence by checking $\rank(M:N)$.}\label{algo:checking linear independence by checking rank(M:N)}
    \KwData{A list of boundary operators $U_1,\ldots, U_n$}
    \KwResult{Whether they are linearly independent}
    \Begin{
        $M, N \longleftarrow$ $n\times n$ matrices associated with $U=(U_1,\ldots,U_n)$ defined in Remark \ref{rmk:writing Ux in M, N (M:N)}\;
        mat $ \longleftarrow (M:N)$\

        \Return{$\rank(\text{mat})==n$}
    }
\end{algorithm}

Extending $U_1,\ldots,U_n$ to $U_1,\ldots,U_{2n}$ is equivalent to imbedding $(M:N)$ in a $2n\times 2n$ nonsingular matrix. Mirroring the approach in the previous section, we may start with any $2n\times 2n$ nonsingular matrix $A$ (for convenience we pick $A=E_{2n}$, the identity matrix) which corresponds to a list of linearly independent boundary forms $V_1,\ldots, V_{2n}$ (\href[page=62]{./textbooks/Linear Algebra Done Right 3rd ed.pdf}{which is a basis for $\mathcal{V}$}), and then reduce some ``combination'' of $(M:N)$ and $A$ to a $2n\times 2n$ nonsingular matrix containing $(M:N)$. That is, writing
\[A=\begin{bmatrix}A_{11} & A_{12}\\ A_{21} & A_{22}\end{bmatrix}\] where all the blocks are $n\times n$ matrices, we seek to turn the $3n\times 2n$ matrix
\[(M:N):A := \begin{bmatrix}M & N\\
A_{11} & A_{12}\\
A_{21} & A_{22}\\\end{bmatrix}\]
into the $2n\times 2n$ matrix with rank $2n$
\[\begin{bmatrix}M&N\\ A_{11}' & A_{12}'\end{bmatrix}.\]
Since $\rank(A)=2n$, $(M:N):A$ has at least $2n$ linearly independent rows, namely its row $n$ to row $3n$ which come from $A$. But row rank is equal to column rank and $(M:N):A$ only has $2n$ columns. So $\rank((M:N):A)=2n$. This means that we can remove $n$ rows from $(M:N):A$ without changing its rank, thus obtaining the desired $2n\times 2n$ matrix. 

To find which rows to remove in $(M:N):A$, we could customize the row echelon form algorithm so that the row operations do not overwrite the first $n$ rows.

% Recall that row elementary operations preserve the linear dependence relation among columns but not among rows (i.e., when rows are interchanged). And columns with pivot in them are linearly independent. So to find which rows to remove in $(M:N):A$, we could first reduce $[(M:N):A]^T$ to its row echelon form; then the columns without pivot in $[(M:N):A]^T$ are rows to be removed in $(M:N):A$. No rows from $(M:N)$ would be removed because the pivots are the left-most non-zero entries in their respective rows in $[(M:N):A]^T$. This means that all non-pivot columns in $[(M:N):A]^T$ would be at the right, i.e., all rows to be removed in $(M:N):A$ would be at the bottom. 

% \begin{algorithm}[H]
%     \caption{Removing rows from $(M:N):A$ without changing rank: Elementary operations.}\label{algo:removing rows from (M:N):A without changing rank: elementary operations.}
%     \KwData{$(M:N):A_{3n\times 2n}$ with rank $2n$}
%     \KwResult{A $2n\times 2n$ matrix with rank $2n$ where the first $n$ rows are $(M:N)$}
%     \Begin{
%         mat $\longleftarrow$ $(M:N):A$\;
%         matT $\longleftarrow$ transpose(mat)\;
%         matTRref $\longleftarrow$ rref(matT); \Comment*[r]{rref := reduced row echelon form}\
%         \For{i in range(ncol(matTRref))}{
%             \If{matTRef[,i] contains no pivot}{
%                 mat $\longleftarrow$ mat[-i,]
%             }
%         }
%         \Return{mat}
%     }
% \end{algorithm}

Alternatively, we can append the rows of $A$ one by one to $(M:N)$ and discard any row that do not make the rank of the resulting matrix increase.

\begin{algorithm}[H]
    \caption{Removing rows from $(M:N):A$ without changing rank: Append and check rank.}\label{algo:removing rows from (M:N):A without changing rank by checking rank: apppend and check rank.}
    \KwData{$(M:N)_{n\times 2n}$ with rank $n$, $A_{2n\times 2n}$ with rank $2n$}
    \KwResult{A $2n\times 2n$ matrix with rank $2n$ where the first $n$ rows are $(M:N)$}
    \Begin{
        mat $\longleftarrow$ $(M:N)$\;
        \For{i in range(nrow(A))}{
            mat1 $\longleftarrow$ vcat(mat, A[i,]) \Comment*[r]{vcat := vertical concatenation}\
            \If{rank(mat1) == rank(mat)+1}{
                mat $\longleftarrow$ mat1
            }
            \Else{
                A $\longleftarrow$ A[-i,]
            }
        }
        \Return{vcat(mat, A)}
    }
\end{algorithm}

% Thus, if we reduce $(M:N):A$ to its row echelon form, we would have exactly $n$ all-zero rows. Remove the corresponding rows in $(M:N):A$, we would get the desired $2n\times 2n$ matrix. 
% \unsure[inline]{Since elementary operations can interchange rows, do the positions of the all-zero rows in the row echelon form correspond to the positions of the linearly dependent rows in the original matrix? If not, what ensures that none of the rows from $(M:N)$ would be removed?}
% \unsure[inline]{However, if the second answer \href{https://math.stackexchange.com/questions/1624238/reduced-row-echelon-form-and-linear-independence}{here} is true in general (leading coefficient shows which columns in the original matrix are independent), perhaps we could work with $[(M:N):A]^{T}$ instead? I.e., on the row echelon form of $[(M:N):A]^{T}$, get the linearly independent columns, which are the linearly independent rows in $(M:N):A$.}
% \unsure[inline]{Is this whole argument reasonable? If so, does the same argument apply when $M$, $N$ have complex entries?} 

We then identify it with
\[H = \begin{bmatrix}M&N\\ \tilde{M} & \tilde{N}\end{bmatrix}\]
and define
\[U_c x = \tilde{M}\xi(a) + \tilde{N}\xi(b).\]
Following the steps in the previous section gives $U^+$.
\end{document}

