\documentclass[11pt, oneside, a4paper]{article}
\input{preamble.tex}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{physics}
%\usepackage{float}
\usepackage[margin=1.in]{geometry}
\usepackage{tocloft}
\renewcommand{\cfttoctitlefont}{\hfill\Large\bfseries}
\renewcommand{\cftaftertoctitle}{\hfill\hfill}
\renewcommand{\cftloftitlefont}{\hfill\Large\bfseries}
\renewcommand{\cftafterloftitle}{\hfill}
\renewcommand{\cftlottitlefont}{\hfill\Large\bfseries}
\renewcommand{\cftafterlottitle}{\hfill}
% These 6 lines will center the titles of the table of contents, list of figures and list of tables

\setcounter{tocdepth}{5}    % how many sectioning levels to show in ToC
% --------------------------------------------------------------------------
\begin{document}
\begin{titlepage}
\pagenumbering{gobble}
% \addcontentsline{toc}{section}{Abstract}
\begin{center}
\vspace{1cm}
\huge
\textbf{Algorithmic Solution of\\ High Order Partial Differential Equations\\
in Julia\\ via the Fokas Transform Method}

\LARGE
\vspace{.5cm}
An Initial Thesis Presented in Partial Fulfillment of\\ the Honours Bachelor's Degree in Science\\
\vspace{.5cm}
\textbf{Linfan Xiao}\\
\vspace{.5cm}
\Large
\vspace{.5cm}
\Large
% \textbf{Abstract}
\end{center}
% TBD

\vfill
\begin{center}	
Mathematical, Computational, and Statistical Sciences\\
Under the supervision of Prof. Dave Smith\\
Yale-NUS College\\
March 2019\\
\end{center}
\end{titlepage}
\pagenumbering{gobble}
\tableofcontents
\listoffigures
% \listoftables
\pagebreak
\pagenumbering{gobble}
% \begin{center}\section*{Acknowledgments}\end{center}
% \addcontentsline{toc}{section}{Acknowledgments}
% TBD

\pagebreak
\pagenumbering{arabic}

\section{Introduction}
Evolution partial differential equations (PDEs) relate a quantity to its rates of change with respect to both time and position. Evolution PDEs can model a variety of phenomena in physics, such as wave propagation and particle motion. This project concerns implementing an algorithmic procedure to solve a certain class of initial-boundary value problems (IBVP) for evolution PDEs in the finite interval \cite{Smith2016} based on the Fokas transform method \cite{Fokas2008}. The implementation is done in Julia, with a focus on symbolic results among numeric features.

\subsection{Motivation}
To motivate the project, suppose we want to solve some IBVPs.

Certain IBVPs (henceforth referred to as type I problems) can be solved algorithmically via classical transform pairs such as the Fourier transform (Figure \ref{fig:classical_transform}). 
\begin{figure}[htpb!]
    \centering
    \includegraphics[width=0.8\textwidth]{classical_transform.png}
    \caption{Solving type I IBVPs using classical transform pairs such as the Fourier transform.}
    \label{fig:classical_transform}
\end{figure}

\noindent For example, the heat equation \cite{Pinsky1991}
\[u_t = au_{xx},\quad (x,t)\in (0, L)\times (0,\infty)\]
with periodic boundary conditions and initial condition
\begin{alignat*}{3}
    u(0, t) &= u(L, t),&\quad t&\in (0,\infty)\\
    u_x(0, t) &= u_x(L, t),&\quad t&\in (0,\infty)\\
    u(x,0) &= f(x),&\quad x&\in (0, L)
\end{alignat*}
can be turned into an ordinary differential equation (ODE) in the temporal variable $t$
\[U_t = -a\cdot s^2U\]
with initial conditions 
\[U(s,0) = F(s),\quad s\in (0,L).\]
where $U(s,t):=\mathcal{F}[u]$ and $F(s):=\mathcal{F}[f]$ are the Fourier transforms of $u(x,t)$ and $f(x)$, respectively, with respect to $x$. Similarly, the linear Schr\"{o}dinger equation \cite{Taylor2018} for a free particle given by 
\[ih w_t(x,t) = -\frac{h^2}{2m}w_{xx}(x,t)\]
with periodic boundary conditions can be turned into an ODE
\[W_t(s,t) = \frac{h}{2mi} s^2W(s,t),\]
where $W(s,t):=\mathcal{F}[w]$ is the Fourier transform of the wave function $w(x,t)$.

For more complicated IBVPs (henceforth referred to as type II problems), however, no such classical transform pairs exist. And solving these IBVPs typically requires a combination of ad-hoc methods. These methods are often specific to the given problem and cannot be generalized to problems with different parameters (e.g., IBVP involving PDE of a different order or different boundary conditions).

The Fokas transform method \cite{Fokas2008} extends the idea of transform pairs to solving type II problems by constructing non-classical transform pairs based on the problem parameters. Since appropriate transform pairs can now be ``customized'' for IBVPs with different parameters, this means that the Fokas method allows solving an entire class of IBVPs algorithmically in a manner similar to Figure \ref{fig:classical_transform} (Figure \ref{fig:fokas_transform}).
\begin{figure}[htpb!]
    \centering
    \includegraphics[width=0.8\textwidth]{fokas_transform.png}
    \caption{Solving type II IBVPs using the Fokas transform pairs.}
    \label{fig:fokas_transform}
\end{figure}

\noindent Examples of IBVPs that can be solved by the Fokas method include second order PDEs of the form
\[q_t(x,t) + a(-i)^2 q_{xx}(x,t) = 0,\]
with boundary conditions
\begin{equation}\label{eqn:informal_example}
    \begin{split}
    q(0,t) + c_0q(1,t) &= 0\\
    q_x(0,t) + c_1q_x(1,t) &= 0,
    \end{split}
\end{equation}
or
\begin{align*}
    q(0,t) + c_0 q_x(0,t) &= 0\\
    q(1) + c_1 q_x(1,t) &= 0,
\end{align*}
where $c_0, c_1\in\hat{\C}:=\C\cup\{0,\infty\}$ (in the sense that if $c_0=\infty$ in \ref{eqn:informal_example}, then dividing by $c_0$ makes $q(0,t)$ disappear and leaves the first equation as $q(1,t) = 0$), as well as third order PDEs of the form 
\[q_t(x,t) \pm i(-i)^2 q_{xxx}(x,t) = 0,\]
with boundary conditions
\begin{align*}
    q(0,t) + c_0 q(1,t) &= 0\\
    q_x(0,t) + c_1 q_x(1,t) &= 0\\
    q_{xx}(0,t) + c_2 q_{xx}(1,t) &= 0,
\end{align*}
where $c_j\in\hat{\C}$ for $j=0,1,2$, or 
\begin{align*}
    q(0,t) &= 0\\
    q(1,t) &= 0\\
    q_x(0,t) + c q_x(1,t) &= 0,
\end{align*}
where $c\in\hat{\C}$. 

In fact, as we will see later, the Fokas method is applicable to IBVPs in the above form with PDEs of any general order $n$. In general, compared to PDEs of higher orders, first and second order PDEs are studied and applied more frequently. The Fokas method advances the understanding of PDEs with higher orders by providing an algorithmic way to solve an entire class of IBVPs of arbitrary order. Using the Fokas method by hand, however, is laborious. It is thus of interest to implement the Fokas method as a software package that aims to supply as much computer aid as possible in the computation process. Examples of such computer aid include visualizations of integration contours, numerical evaluations, and deriving explicit symbolic formulae for important objects which would help proving technical lemmas that arise when applying the Fokas method to different types of problems \cite{Miller2018}. 

This project is developed in Julia, a free open-source, high-performance language for numerical computing \cite{julia}. Among tools for numerical computing, commercial softwares may provide functionalities to execute sophisticated computational tasks; yet due to being closed-source, there exists a lack of transparency which limits users' understanding of the nature of the software output. As a result, there has been a trend in the mathematical community that encourages the use of open-source languages like Julia, which allows users to verify the correctness of the output. Therefore, we choose Julia to be the language of implementation. To our knowledge, despite the presence of various differential equation solvers in mathematical softwares such as Mathematica and Matlab, this is the first time that the Fokas method is implemented computationally, which allows solving a particular class of IBVPs for which no other solver algorithm yet exists.

\subsection{The Initial-Boundary Value Problem}
To formally characterize the class of IBVPs that can be solved by the Fokas method \cite[p.9]{Smith2016}, we first introduce the following definitions.
\begin{itemize}
    \item Define linearly independent boundary forms $B_j: C^\infty[0,1]\to \C$ (where $C^\infty[0,1]$ denotes the class of real-valued functions differentiable for all orders on $[0,1]$) by
    \begin{equation}\label{eqn:B_j}
        B_j\phi := \sum_{k=0}^{n-1}\left(b_{jk}\phi^{(k)}(0) + \beta_{jk}\phi^{(k)}(1)\right),\, j\in\{1,2,\ldots,n\}
    \end{equation}
    where the boundary coefficients $b_{jk}$, $\beta_{jk}\in\R$. 
    \item Define
    \begin{equation}\label{eqn:Phi}
        \Phi:=\{\phi\in C^\infty[0,1]:\, B_j\phi = 0\,\forall j\in\{1,2,\ldots,n\}\}
    \end{equation}
    to be the set of smooth functions $\phi$ satisfying the homogeneous boundary conditions $B_j\phi=0$ for $j\in\{1,2,\ldots,n\}$.
    \item Define 
    \[S = (-i)^n \frac{d^n}{dx^n}\]
    to be a spatial differential operator of order $n$ (i.e., a differential operator in the spatial variable $x$).
\end{itemize}

The Fokas method allows solving any well-posed IBVP that can be written in the form
\begin{alignat}{3}
    (\partial_t + aS)q(x,t) &= 0\quad &\forall (x,t)&\in (0,1)\times (0,T) \label{eqn:PDE}\\
    q(x,0) &= q_0(x)\in \Phi\quad &\forall x&\in [0,1]\label{eqn:initial_condition}\\
    q(\cdot, t) &= f \in \Phi \quad &\forall t&\in [0,T],\label{eqn:boundary_condition}
\end{alignat}
where $a$ is a complex constant. For the IBVP to be well-posed, we require that $a=\pm i$ if $n$ is odd and $\real(a)\geq 0$ if $n$ is even. Equation \ref{eqn:PDE} is a PDE relating a quantity $q$ to its temporal and spatial rates of change $\partial_t[q]$ and $aS[q]$. \ref{eqn:initial_condition} is the initial condition where the temporal variable $t=0$. \ref{eqn:boundary_condition} corresponds to the homogeneous spatial boundary conditions. 
An example of such IBVPs is the linear Schr\"{o}dinger equation \cite{Taylor2018}, which describes the wave form of a quantum system in free space. The linear Schr\"{o}dinger equation with zero potential is given by
\[ih\frac{\partial}{\partial t}w + \frac{h^2}{2m}\frac{\partial^2}{\partial x^2}w = 0,\]
where $w(x,t)$ is the wave function, $h$ is the Planck's constant, $m$ is the mass of the particle, and $kx$ describes the potential energy of the particle in the force field. It can be written in the form of \ref{eqn:PDE} as
\[(\partial_t + aS)q(x,t) = 0\]
where $q=w$, $S = \frac{\partial^2}{\partial x^2}$, and $a = \frac{h}{2mi}$. The initial condition 
\[w(x,0) = w_0(x)\]
would mean that the system's wave form at initial stage is described by the wave function $w_0(x)$, and the boundary conditions 
\[w(\cdot, t) = f\]
would mean that the behaviour of the system at its boundary is described by the function $f$ (e.g., if we are considering particles moving in a box, $f$ would describe how the particles behave at the box's boundaries).

For appropriate transform pair $f(x)=f_x(F)$ and $F(\lambda)=F_\lambda(f)$ found using the Fokas method, the solution to the IBVP characterized above is given by \cite[p.15]{Smith2016}
\begin{equation}\label{eqn:q(x,t)}
    q(x,t) = f_x(e^{-a\lambda^n t}F_\lambda(f)),
\end{equation}
completing Figure \ref{fig:fokas_transform}.

% \section{Constructing the Adjoint of a Homogeneous Boundary Condition}
% \subsection{Preliminaries}
% \subsection{Implementation}

% \section{The Fokas Transform Pair}
% \subsection{Preliminaries}
% \subsection{Implementation}

\section{Preliminaries}
We now begin to introduce the preliminary definitions and results used in the algorithmic procedure that finds \ref{eqn:q(x,t)}.

\subsection{Homogeneous Boundary Value Problems}
\begin{defn}\cite[p.81]{CoddingtonLevinson}\label{defn:linear differential operator}
    A \textbf{linear differential operator} $L$ of order $n$ ($n>1$) on interval $[a,b]$ is defined by
    \[Lx = p_0x^{(n)} + p_1x^{(n-1)} + \cdots + p_{n-1}x' + p_nx,\]
    where the $p_k$ are complex-valued functions of class $C^{n-k}$ on $[a,b]$ and $p_0(t)\neq 0$ on $[a,b]$.
\end{defn}

\begin{defn}\cite[p.284]{CoddingtonLevinson}\label{defn:homogeneous boundary conditions}
    \textbf{Homogeneous boundary conditions} refer to a set of equations of the type
    \begin{equation}\label{eq:homogeneous boundary conditions}
        \sum_{k=1}^n (M_{jk}x^{(k-1)}(a) + N_{jk}x^{(k-1)}(b))=0 \quad (j=1,\ldots,m) 
    \end{equation}
    where $M_{jk}, N_{jk}$ are complex constants.
\end{defn}

\begin{defn}\cite[p.284]{CoddingtonLevinson}\label{defn:homogeneous boundary value problem}
    A \textbf{homogeneous boundary value problem} concerns finding the solutions of 
    \[Lx=0\] on some interval $[a,b]$ which satisfy some homogeneous boundary conditions.
\end{defn}

\subsection{Adjoints of Homogeneous Boundary Value Problems}

Recall from Linear Algebra that, given a linear map $T$ from inner product spaces $V$ to $W$ (denoted as $T\in\mathcal{L}(V,W)$), the adjoint of $T$ is the function $T^\star:W\to V$ with
\begin{equation}\label{eq:linear map adjoint}
    \langle Tv, w\rangle = \langle v, T^\star w\rangle
\end{equation}
for $v\in V$, $w\in W$, where $\langle \cdot, \cdot \rangle$ denotes inner products defined on $V$ and $W$ \cite[p.204]{Axler1997}. There exists a similar notion for boundary value problems, which we begin to characterize below.

\subsubsection{Adjoint Differential Operator}
\begin{defn}\cite[p.84]{CoddingtonLevinson}\label{defn:adjoint linear differential operator}
    Given a linear differential operator $L$ of order $n$ as in Definition \ref{defn:linear differential operator}, the operator $L^+$ given by
    \[L^+x = (-1)^n (\bar{p}_0 x)^{(n)} + (-1)^{n-1}(\bar{p}_1 x)^{(n-1)} +\cdots +\bar{p}_nx\]
    (where $\bar{p}_k$ is the complex conjugate of $p_k$ for $k\in\{0,\ldots,n\}$) is the \textbf{adjoint} of $L$.
\end{defn}

\subsubsection{Adjoint Boundary Conditions}
For a homogeneous boundary value problem $\pi$ with linear differential operator $L$ and some (homogeneous) boundary conditions, an adjoint problem $\pi^+$ involves the adjoint linear differential operator $L^+$ and some (homogeneous) adjoint boundary conditions. The adjoint boundary conditions are such that an equation similar to (\ref{eq:linear map adjoint}) exists for solutions of $\pi$ and those of $\pi^+$, with the inner product $\langle\cdot, \cdot\rangle$ defined as $\langle u,v\rangle := \int_a^b u\bar{v}\,dt$ for $u, v\in C^n$ on $[a,b]$. In the following sections, we seek to characterize the adjoint boundary conditions.

The characterization depends on two important results, namely Green's formula and the boundary-form formula. Generally speaking, Green's formula allows characterizing a form, which, when used in the boundary-form formula, gives rise to the desired characterization. 

We begin with the Green's formula.

\paragraph{Green's formula}
\begin{thm}\cite[p.284]{CoddingtonLevinson}{(Green's formula)}\label{thm:green's formula}
    For $u, v\in C^n$ on $[a,b]$,
    \begin{equation}\label{eq:green's formula}
        \int_{t_1}^{t_2}(Lu)\bar{v}\,dt - \int_{t_1}^{t_2}u(\overline{L^+v})\,dt = [uv](t_2) - [uv](t_1) 
    \end{equation}
    where $a\leq t_1<t_2\leq b$ and $[uv](t)$ is the form in $(u, u', \ldots, u^{(n-1)})$ and $(v, v', \ldots, v^{(n-1)})$ given by
    \begin{equation}\label{eq:[uv](t) defn}
        [uv](t)=\sum_{m=1}^n\sum_{j+k=m-1}(-1)^j u^{(k)}(t)(p_{n-m}\bar{v})^{(j)}(t)
    \end{equation}
\end{thm}
Using the form $[uv](t)$, we define an important $n\times n$ matrix $B$ whose entries $B_{jk}$ satisfy
\begin{equation}\label{eq:[uv](t) in B matrix}
    \begin{split}
        [uv](t) = \sum_{j,k=1}^n B_{jk}(t)u^{(k-1)}(t)\bar{v}^{(j-1)}(t).
    \end{split}
\end{equation}
Note that
\begin{align*}
    [uv](t) &= \sum_{m=1}^n\sum_{j+k=m-1}(-1)^j u^{(k)}(t)(p_{n-m}\bar{v})^{(j)}(t)\\
        &= \sum_{m=1}^n\sum_{j+k=m-1}(-1)^j u^{(k)}(t)\left(\sum_{\ell=0}^j\binom{j}{\ell}p_{n-m}^{(j-\ell)}(t)\bar{v}^{(\ell)}(t)\right)\\
        &= \sum_{m=1}^n\sum_{k=0}^{m-1}(-1)^{m-1-k} u^{(k)}(t)\left(\sum_{\ell=0}^{m-1-k}\binom{m-1-k}{\ell}p_{n-m}^{(m-1-k-\ell)}(t)\bar{v}^{(\ell)}(t)\right)\\
        &= \sum_{m=1}^n\sum_{k=1}^{m}(-1)^{m-k}\left(\sum_{\ell=0}^{m-k}\binom{m-k}{\ell}p_{n-m}^{(m-k-\ell)}(t)\bar{v}^{(\ell)}(t)\right)u^{(k-1)}(t)\quad\mbox{(shifting $k$ to $k+1$)}\\
        &= \sum_{k=1}^n\sum_{m=k}^n (-1)^{m-k}\left(\sum_{\ell=0}^{m-k}\binom{m-k}{\ell}p_{n-m}^{(m-k-\ell)}(t)\bar{v}^{(\ell)}(t)\right)u^{(k-1)}(t).
\end{align*}

To find $B_{jk}$, we need to extract the coefficients of $u^{(k-1)}\bar{v}^{(j-1)}$. We first note that, fixing $m$ and $k$, when $\ell=j-1$, the coefficient of $\bar{v}^{(j-1)}$ is
\begin{align*}
    \binom{m-k}{j-1}p_{n-m}^{(m-k-j+1)}(t).
\end{align*}
To find the coefficient of $u^{(k-1)}\bar{v}^{(j-1)}$, we need to fix $k$ and collect the above coefficient across all values of $m$. Since $m$ goes up to $n$, $m-k$ goes up to $n-k$. Since $\ell\leq m-k$, $\ell=j-1$ implies $j-1\leq m-k$. Thus, $m-k$ ranges from $j-1$ to $n-k$. Let $\ell':=m-k$, then $m=k+\ell'$, and the above equation becomes
\begin{align*}
    [uv](t) 
    &= \sum_{k=1}^n\sum_{j=1}^n (-1)^{\ell'}\left(\sum_{\ell'=j-1}^{n-k}\binom{\ell'}{j-1}p_{n-(k+\ell')}^{(\ell'-(j-1))}(t)\right)\bar{v}^{(j-1)}(t)u^{(k-1)}(t)\\
    &= \sum_{j,k=1}^n \left(\sum_{\ell=j-1}^{n-k}\binom{\ell}{j-1}p^{(\ell-j+1)}_{n-k-\ell}(t)(-1)^\ell\right)u^{(k-1)}(t)\bar{v}^{(j-1)}(t)\quad\mbox{(replace $\ell'$ by $l$)}.
\end{align*}
Thus,
\[B_{jk}(t) = \sum_{\ell=j-1}^{n-k}\binom{\ell}{j-1}p^{(\ell-j+1)}_{n-k-\ell}(t)(-1)^\ell.\]
We note that for $j+k>n+1$, or $j-1>n-k$, $l$ is undefined. This means that terms $u^{(k-1)}(t)\bar{v}^{(j-1)}(t)$ with $j+k>n+1$ does not exist in $[uv](t)$. Thus, $B_{jk}(t)=0$. Also, for $j+k=n+1$, or $j-1=n-k$,
\[B_{jk}(t) = \binom{j-1}{j-1}p^{(j-1-j+1)}_{j-1-(j-1)}(t)(-1)^{j-1} = (-1)^{j-1}p_0(t).\] 
Thus, the matrix $B$ has the form
\begin{equation}\label{eq:B(t)}
    B(t)=\begin{bmatrix}
        B_{11} & B_{12} & \cdots & \cdots & B_{1\,n-1} & p_0(t)\\
        B_{21} & B_{22} & \cdots & \cdots & -p_0(t) & 0\\
        \vdots & \vdots & \ddots &  & \vdots & \vdots\\
        \vdots & \vdots &  & \ddots & \vdots & \vdots\\
        (-1)^{n-1}p_0(t) & 0 & \cdots & \cdots & 0 & 0
    \end{bmatrix}.
\end{equation}
We note that because $p_0(t)\neq 0$ on $[a,b]$ (as required in Definition \ref{defn:linear differential operator}), $B(t)$ is square with $\det B(t)=(p_0(t))^n\neq 0$ on $[a,b]$. Thus, $B(t)$ is nonsingular for $t\in [a,b]$.


Now we seek another matrix $\hat{B}$ that embodies both the characteristics of $B$ and those of the interval $[a,b]$. This concerns writing the right-hand side of Green's formula in matrix form. We begin by introducing the following definitions.

\begin{defn}\cite[p.285]{CoddingtonLevinson}\label{defn:f cdot g}
    For vectors $f=(f_1,\ldots,f_k)$, $g=(g_1,\ldots,g_k)$, define the product
    \[f\cdot g:=\sum_{i=1}^k f_i\bar{g}_i.\]
    Note that $f\cdot g = g^*f$ where $^*$ denotes conjugate transpose.
\end{defn}

\begin{defn}\cite[p.285]{CoddingtonLevinson}\label{defn:semibilinear form}
    A \textbf{semibilinear form} is a complex-valued function $\mathcal{S}$ defined for pairs of vectors $f=(f_1,\ldots,f_k)$, $g=(g_1,\ldots,g_k)$ satisfying
    \begin{align*}
        \mathcal{S}(\alpha f+\beta g, h)&=\alpha\mathcal{S}(f,h) + \beta\mathcal{S}(g,h)\\
        \mathcal{S}(f, \alpha g + \beta h) &= \bar{\alpha}\mathcal{S}(f,g) + \bar{\beta}\mathcal{S}(f, h)
    \end{align*}
    for any complex numbers $\alpha, \beta$ and vectors $f,g,h$.
\end{defn}
We note that if
\[S = \begin{bmatrix}
    s_{11} & \cdots & s_{1k}\\
    \vdots &  & \vdots\\
    s_{k1} & \cdots & s_{kk}
\end{bmatrix},\]
then $Sf\cdot g$ is given by
\begin{equation}\label{eq:semibilinear form}
    \begin{split}
    \mathcal{S}(f,g) &:= Sf\cdot g = \begin{bmatrix}
        s_{11} & \cdots & s_{1k}\\
        \vdots & & \vdots\\
        s_{k1} & \cdots & s_{kk}
    \end{bmatrix} \begin{bmatrix}
        f_1\\
        \vdots\\
        f_k
    \end{bmatrix} \cdot \begin{bmatrix}
        g_1\\
        \vdots\\
        g_k
    \end{bmatrix} = \begin{bmatrix}
        \sum_{j=1}^k s_{1j}f_j\\
        \vdots\\
        \sum_{j=1}^k s_{kj}f_j
    \end{bmatrix}\cdot \begin{bmatrix}
        g_1\\
        \vdots\\
        g_k
    \end{bmatrix}\\
    &= \sum_{i=1}^k\left(\sum_{j=1}^k s_{ij}f_j\right)\bar{g}_i =\sum_{i,j=1}^k s_{ij}f_i\bar{g}_i.
    \end{split}
\end{equation}
To see that this is a semibilinear form:
\begin{align*}
    \mathcal{S}(\alpha f+\beta g, h)
    &= \sum_{i,j=1}^k s_{ij}(\alpha f_j + \beta g_j)\bar{h}_i
    = \alpha \sum_{i,j=1}^k s_{ij}f_j\bar{h}_i + \beta \sum_{i,j=1}^k g_j\bar{h}_i\\
    &= \alpha Sf\cdot h + \beta Sg\cdot h
    = \alpha\mathcal{S}(f,h) + \beta\mathcal{S}(g,h);
\end{align*}
and similarly,
\begin{align*}
    \mathcal{S}(f, \alpha g + \beta h)
    &= \sum_{i,j=1}^k s_{ij}f_j(\overline{\alpha g_i + \beta h_i})
    = \bar{\alpha}\sum_{i,j=1}^k s_{ij}f_j\bar{g}_i + \bar{\beta}\sum_{i,j=1}^k f_j \bar{h}_i\\
    &= \bar{\alpha}Sf\cdot g + \bar{\beta}Sf\cdot h
    = \bar{\alpha}\mathcal{S}(f,g) + \bar{\beta}\mathcal{S}(f,h).
\end{align*}

Under a similar matrix framework, we see that $[uv](t)$ is a semibilinear form with matrix $B(t)$: Let $\vec{u}=(u, u', \ldots, u^{(n-1)})$ and $\vec{v}=(v, v', \ldots, v^{(n-1)})$. Then we have
\begin{equation}\label{[uv](t) in semibilinear form}
    \begin{split}
    [uv](t) &= \sum_{j,k=1}^n B_{jk}(t)u^{(k-1)}(t)\bar{v}^{(j-1)}(t)\quad\mbox{(by \eqref{eq:[uv](t) in B matrix})}\\
    &= \sum_{i,j=1}^n (B_{ij}u^{(j-1)}\overline{v}^{(i-1)})(t)\\
    &= (B\vec{u}\cdot \vec{v})(t)\quad\mbox{(by (\ref{eq:semibilinear form}))}\\
    &=: \mathcal{S}(\vec{u},\vec{v})(t).
    \end{split}
\end{equation}
With this notation, we can rewrite the right-hand side of Green's formula as a semibilinear form below:
\begin{equation}\label{eq:green's formula in semibilinear form}
    \begin{split}
    [uv](t_2)-[uv](t_1) &= \sum_{j,k=1}^n B_{jk}(t_2)u^{(k-1)}(t_2)\bar{v}^{(j-1)}(t_2) - \sum_{j,k=1}^n B_{jk}(t_2)u^{(k-1)}(t_1)\bar{v}^{(j-1)}(t_1)\\
    &= B(t_2)\vec{u}(t_2)\cdot \vec{v}(t_2) - B(t_1)\vec{u}(t_1)\cdot \vec{v}(t_1)\\
    &= \begin{bmatrix}
        B_{11}(t_2) & \cdots & B_{1n}(t_2)\\
        \vdots &  & \vdots\\
        B_{n1}(t_2) & \ldots & B_{nn}(t_2)
    \end{bmatrix} 
    \begin{bmatrix}
    u(t_2)\\
    \vdots\\
    u^{(n-1)}(t_2)
    \end{bmatrix}\cdot 
    \begin{bmatrix}
        \bar{v}(t_2)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_2)
    \end{bmatrix} -\\
    &\qquad \begin{bmatrix}
        B_{11}(t_1) & \cdots & B_{1n}(t_1)\\
        \vdots &  & \vdots\\
        B_{n1}(t_1) & \ldots & B_{nn}(t_1)
    \end{bmatrix} 
    \begin{bmatrix}
    u(t_1)\\
    \vdots\\
    u^{(n-1)}(t_1)
    \end{bmatrix}\cdot 
    \begin{bmatrix}
        \bar{v}(t_1)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_1)
    \end{bmatrix}\\
    &= \begin{bmatrix}
        -B_{11}(t_1) & \cdots & -B_{1n}(t_1)\\
        \vdots &  & \vdots\\
        -B_{n1}(t_1) & \ldots & -B_{nn}(t_1)
    \end{bmatrix} 
    \begin{bmatrix}
    u(t_1)\\
    \vdots\\
    u^{(n-1)}(t_1)
    \end{bmatrix}\cdot 
    \begin{bmatrix}
        \bar{v}(t_1)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_1)
    \end{bmatrix} + \\
    &\qquad \begin{bmatrix}
        B_{11}(t_2) & \cdots & B_{1n}(t_2)\\
        \vdots &  & \vdots\\
        B_{n1}(t_2) & \ldots & B_{nn}(t_2)
    \end{bmatrix} 
    \begin{bmatrix}
    u(t_2)\\
    \vdots\\
    u^{(n-1)}(t_2)
    \end{bmatrix}\cdot 
    \begin{bmatrix}
        \bar{v}(t_2)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_2)
    \end{bmatrix}\\
    &= \begin{bmatrix}
        -B_{11}(t_1) & \cdots & -B_{1n}(t_1) & 0 & \cdots & 0\\
        \vdots &  & \vdots & \vdots &  & \vdots\\
        -B_{n1}(t_1) & \cdots & -B_{nn}(t_1) & 0 & \cdots & 0\\
        0 & \cdots & 0 & B_{11}(t_2) & \cdots & B_{1n}(t_2)\\
        \vdots &  & \vdots & \vdots &  & \vdots\\
        0 & \cdots & 0 & B_{n1}(t_2) & \cdots & B_{nn}(t_2)
    \end{bmatrix} 
    \begin{bmatrix}
        u(t_1)\\
        \vdots\\
        u^{(n-1)}(t_1)\\
        u(t_2)\\
        \vdots\\
        u^{(n-1)}(t_2)
    \end{bmatrix}\cdot
    \begin{bmatrix}
        \bar{v}(t_1)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_1)\\
        \bar{v}(t_2)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_2)
    \end{bmatrix}\\
    &= \begin{bmatrix}
        -B(t_1) & 0_n\\
        0_n & B(t_2)
    \end{bmatrix}
    \begin{bmatrix}
        u(t_1)\\
        \vdots\\
        u^{(n-1)}(t_1)\\
        u(t_2)\\
        \vdots\\
        u^{(n-1)}(t_2)
    \end{bmatrix}\cdot
    \begin{bmatrix}
        \bar{v}(t_1)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_1)\\
        \bar{v}(t_2)\\
        \vdots\\
        \bar{v}^{(n-1)}(t_2)
    \end{bmatrix}\\
    &=:\hat{B}
    \begin{bmatrix}
        \vec{u}(t_1)\\
        \vec{u}(t_2)
    \end{bmatrix}\cdot
    \begin{bmatrix}
        \vec{v}(t_1)\\
        \vec{v}(t_2)
    \end{bmatrix}.
\end{split}
\end{equation}
Recall that $\det(\lambda A) = \lambda^n\det(A)$ for $n\times n$ matrix $A$. Thus,
\[\det\hat{B}= \det(-B(t_1))\det(B(t_2))= (-1)^n\det B(t_1)\det B(t_2)\]
since $B(t_1)$ is $n\times n$. Since $B(t)$ is nonsingular for $t\in [a,b]$ (as shown before), $\hat{B}$ is nonsingular for $t_1, t_2\in [a,b]$.

To recapitulate, given a linear differential operator $L$ which involves functions $p_0,\ldots,p_n$ and an interval $[a,b]$, from the Green's formula, we have defined a matrix $B$ which depends on $p_0,\ldots,p_n$ and a matrix $\hat{B}$ which depends on $B$ and $[a,b]$. These objects will be important in characterizing an adjoint boundary condition using the boundary-form formula, which we now turn to.

\paragraph{Boundary-form Formula}
Before introducing the boundary-form formula, we need a set of definitions and results concerning boundary conditions.

\begin{defn}\cite[p.286]{CoddingtonLevinson}\label{defn:boundary form}
    Given any set of $2mn$ complex constants $M_{ij}, N_{ij}$ ($i=1,\ldots, m;\,j=1,\ldots,n$), define $m$ \textbf{boundary operators (boundary forms)} $U_1,\ldots,U_m$ for functions $x$ on $[a,b]$, for which $x^{(j)}$ ($j=1,\ldots,n-1$) exists at $a$ and $b$, by
    \begin{equation}\label{eq:U_i defn}
        U_i x = \sum_{j=1}^n (M_{ij}x^{(j-1)}(a) + N_{ij}x^{(j-1)}(b))\quad\mbox{($i=1,\ldots,m$)} 
    \end{equation}
    $U_i$ are \textbf{linearly independent} if the only set of complex constants $c_1, \ldots, c_m$ for which
    \[\sum_{i=1}^m c_i U_ix=0\]
    for all $x\in C^{n-1}$ on $[a,b]$ is $c_1=c_2=\cdots =c_m=0$.
\end{defn}

\begin{defn}\cite[p.286]{CoddingtonLevinson}\label{defn:vectory boundary form}
    A \textbf{vectory boundary form} $U=(U_1,\ldots,U_m)$ is a vector whose components are boundary forms (Definition \ref{defn:boundary form}). When $U_1,\ldots,U_m$ are linearly independent, we say that $U$ has rank $m$. We assume $U$ has full rank below.
\end{defn}

With the above definitions, we can now write a set of homogeneous boundary conditions (Definition \ref{defn:homogeneous boundary conditions}) in matrix form. Define
\[
    \xi:= \begin{bmatrix}x\\ x'\\ \vdots \\ x^{(n-1)}
    \end{bmatrix};
        \quad
    U := \begin{bmatrix}U_1\\ U_2\\ \vdots \\ U_m
    \end{bmatrix};
        \quad
    M := \begin{bmatrix}
        M_{11} & \cdots & M_{1n}\\
        \vdots &  & \vdots\\
        M_{m1} & \cdots & M_{mn}
    \end{bmatrix};
    \quad
    N := \begin{bmatrix}
        N_{11} & \cdots & N_{1n}\\
        \vdots &  & \vdots\\
        N_{m1} & \cdots & N_{mn}
    \end{bmatrix}.
\]
Then the set of homogeneous boundary conditions in (\ref{eq:homogeneous boundary conditions}) can be written as
\[Ux = M\xi(a) + N\xi(b).\]
Indeed:
\begin{align*}
    M\xi(a) + N\xi(b) &= \begin{bmatrix}
        M_{11} & \cdots & M_{1n}\\
        \vdots &  & \vdots\\
        M_{m1} & \cdots & M_{mn}
    \end{bmatrix}\begin{bmatrix}x(a)\\ x'(a)\\ \vdots \\ x^{(n-1)}(a)
        \end{bmatrix} + \begin{bmatrix}
            N_{11} & \cdots & N_{1n}\\
            \vdots &  & \vdots\\
            N_{m1} & \cdots & N_{mn}
        \end{bmatrix}\begin{bmatrix}x(b)\\ x'(b)\\ \vdots \\ x^{(n-1)}(b)
        \end{bmatrix}\\
        &= \begin{bmatrix}
            \sum_{j=1}^n M_{1j}x^{(j-1)}(a)\\
            \vdots\\
            \sum_{j=1}^n M_{mj}x^{(j-1)}(a)
        \end{bmatrix} + \begin{bmatrix}
            \sum_{j=1}^n N_{1j}x^{(j-1)}(b)\\
            \vdots\\
            \sum_{j=1}^n N_{mj}x^{(j-1)}(b)
        \end{bmatrix}\\
        &= \begin{bmatrix}
            \sum_{j=1}^n (M_{1j}x^{(j-1)}(a) + N_{1j}x^{(j-1)}(b))\\
            \vdots\\
            \sum_{j=1}^n (M_{mj}x^{(j-1)}(a) + N_{mj}x^{(j-1)}(b))
        \end{bmatrix}\\
        &= \begin{bmatrix}
            U_1 x\\
            \vdots\\
            U_m x
        \end{bmatrix} = \begin{bmatrix}U_1\\ U_2\\ \vdots \\ U_m
        \end{bmatrix}x = Ux.
\end{align*}
Based on the above, we propose another way to write $U_x$. Define the $m\times 2n$ matrix
\[(M:N):=\begin{bmatrix}
    M_{11} & \cdots & M_{1n} & N_{11} & \cdots & N_{1n}\\
    \vdots &  & \vdots & \vdots & & \vdots\\
    M_{m1} & \cdots & M_{mn} & N_{m1} & \cdots & N_{mn}
\end{bmatrix}.\]
Then $U_1,\ldots,U_m$ are linearly independent if and only if $\rank(M:N)=m$, or equivalently, $\rank(U)=m$. 
%Recall that the rank of a matrix is the largest number of linearly independent rows or columns in it. For a matrix $A_{m\times n}$, $\rank(A)\leq \min\{m,n\}$ and $\rank(A)=\rank(A^T)$.
Moreover, $Ux$ can also be written as
\begin{align*}
    Ux &= \begin{bmatrix}
        \sum_{j=1}^n (M_{1j}x^{(j-1)}(a) + N_{1j}x^{(j-1)}(b))\\
        \vdots\\
        \sum_{j=1}^n (M_{mj}x^{(j-1)}(a) + N_{mj}x^{(j-1)}(b))
    \end{bmatrix}\\
    &= \begin{bmatrix}
        M_{11} & \cdots & M_{1n} & N_{11} & \cdots & N_{1n}\\
        \vdots &  & \vdots & \vdots & & \vdots\\
        M_{m1} & \cdots & M_{mn} & N_{m1} & \cdots & N_{mn}
    \end{bmatrix} \begin{bmatrix}x(a)\\\vdots\\x^{(n-1)}(a)\\ x(b)\\\vdots\\x^{(n-1)}(b)\end{bmatrix}\\
    &= (M:N)\begin{bmatrix}
        \xi(a)\\
        \xi(b)
    \end{bmatrix}.
\end{align*}

Having proposed a compact way to represent a set of homogeneous boundary conditions, we begin building our way to characterizing the notion of adjoint boundary condition. First, we need the notion of a complementary boundary form.

\begin{defn}\cite[p.287]{CoddingtonLevinson}\label{defn:complementary boundary form}
    If $U=(U_1,\ldots, U_m)$ is any boundary form with $\rank(U)=m$ and $U_c=(U_{m+1},\ldots,U_{2n})$ is any form with $\rank(U_c)=2n-m$ such that $(U_1,\ldots, U_{2n})$ has rank $2n$, then $U$ and $U_c$ are \textbf{complementary boundary forms}. 
    
    Note that extending $U_{1},\ldots, U_{m}$ to $U_1,\ldots,U_{2n}$ is equivalent to embedding the matrix $(M:N)$ in a $2n\times 2n$ nonsingular matrix (recall that a square matrix is nonsingular if and only if it has full rank).
\end{defn}

The characterization of adjoint boundary conditions is given by the boundary-form formula. The boundary-form formula is motivated by writing the right-hand side of Green's formula \eqref{eq:green's formula} as the linear combination of a boundary form $U$ and a complementary form $U_c$. Before finally getting to it, we need the following propositions.

\begin{prop}\cite[p.287]{CoddingtonLevinson}
    In the context of the semibilinear form \eqref{eq:semibilinear form}, we have
    \begin{equation}\label{eq:semibilinear adjoint}
        Sf\cdot g = f\cdot S^*g, 
    \end{equation}
    where $S^*$ is the conjugate transpose of $S$.
\end{prop}
\begin{proof}
    \begin{align*}
        Sf\cdot g &= \sum_{i,j=1}^k s_{ij}f_j\bar{g}_i \quad\mbox{(by \eqref{eq:semibilinear form})};\\
        f\cdot S^*g &= \begin{bmatrix}
            f_1\\
            \vdots\\
            f_k
        \end{bmatrix}\cdot \begin{bmatrix}
            \bar{s}_{11} & \cdots & \bar{s}_{k1}\\
            \vdots & & \vdots\\
            \bar{s}_{1k} & \cdots & \bar{s}_{kk}
        \end{bmatrix} \begin{bmatrix}
            g_1\\
            \vdots\\
            g_k
        \end{bmatrix}\\
        &= \begin{bmatrix}
            f_1\\
            \vdots\\
            f_k
        \end{bmatrix}\cdot \begin{bmatrix}
            \sum_{j=1}^k \bar{s}_{j1}g_j\\
            \vdots\\
            \sum_{j=1}^k \bar{s}_{jk}g_j
        \end{bmatrix}\\
        &= \sum_{i=1}^k f_i\cdot \left(\overline{\sum_{j=1}^k\bar{s}_{ji}g_j}\right)\\
        &= \sum_{i=1}^k f_i\cdot \left(\sum_{j=1}^k s_{ji}\bar{g}_j\right)\\
        &= \sum_{i,j=1}^k s_{ji}f_i\bar{g}_j = Sf\cdot g.\qedhere
    \end{align*}
\end{proof}

\begin{prop}\cite[p.287]{CoddingtonLevinson}\label{prop:unique nonsingular G in semibilinear form}
    Let $\mathcal{S}$ be the semibilinear form associated with a nonsingular matrix $S$. Suppose $\bar{f}:=Ff$ where $F$ is a nonsingular matrix. Then there exists a unique nonsingular matrix $G$ such that if $\bar{g}=Gg$, then $\mathcal{S}(f,g)=\bar{f}\cdot \bar{g}$ for all $f, g$.
\end{prop}
\begin{proof}
    Let $G:=(SF^{-1})^*$, then
    \begin{align*}
        \mathcal{S}(f,g) &= Sf\cdot g\\
        &= S(F^{-1}F)f\cdot g\\
        &= SF^{-1}(Ff)\cdot g\\
        &= SF^{-1}\bar{f} \cdot g\\
        &= \bar{f}\cdot (SF^{-1})^*g \quad\mbox{(by \eqref{eq:semibilinear adjoint})}\\
        &= \bar{f}\cdot G*g\\
        &= \bar{f}\cdot \bar{g}.
    \end{align*}
    To see that $G$ is nonsingular, note that $\det G = \det((\overline{SF^{-1}})^T) = \det(\overline{SF^{-1}}) = \overline{\det(SF^{-1})} = \overline{\det(S)\det(F)^{-1}}\neq 0$ since $S, F$ are nonsingular.
\end{proof}

\begin{prop}\cite[p.287]{CoddingtonLevinson}\label{prop:last k-j components linear combination}
    Suppose $\mathcal{S}$ is associated with the unit matrix $E$, i.e., $\mathcal{S}(f,g)=f\cdot g$. Let $F$ be a nonsingular matrix such that the first $j$ ($1\leq j<k$) components of $\bar{f}=Ff$ are the same as those of $f$. Then the unique nonsingular matrix $G$ such that $\bar{g}=Gg$ and $\bar{f}\cdot \bar{g}=f\cdot g$ (as in Proposition \ref{prop:unique nonsingular G in semibilinear form}) is such that the last $k-j$ components of $\bar{g}$ are linear combinations of the last $k-j$ components of $g$ with nonsingular coefficient matrix.
\end{prop}
\begin{proof}
    We note that for the condition on $F$ to hold, $F$ must have the form
\[\begin{bmatrix}E_j & 0_+\\
F_+ & F_{k-j}\end{bmatrix}_{k\times k}\]
where $E_j$ is the $j\times j$ identity matrix, $0_+$ is the $j\times (k-j)$ zero matrix, $F_+$ is a $(k-j)\times j$ matrix, and $F_{k-j}$ a $(k-j)\times (k-j)$ matrix. Let $G$ be the unique nonsingular matrix in Proposition \ref{prop:unique nonsingular G in semibilinear form}. Write $G$ as
\[\begin{bmatrix}G_j & G_-\\
G_= & G_{k-j}\end{bmatrix}_{k\times k}\]
where $G_j, G_-, G_=, G_{k-j}$ are $j\times j, j\times (k-j), (k-j)\times j, (k-j)\times (k-j)$ matrices, respectively. By the definition of $G$,
\[f\cdot g = Ff\cdot Gg = \bar{f}\cdot Gg = G^*\bar{f}\cdot g = G^*Ff\cdot g,\]
(where the third equality follows from a reverse application of \eqref{eq:semibilinear adjoint} with $\bar{f}$ as $f$, $G^*$ as $S$) which implies
\[G^*F = E_k.\]
Since
\begin{align*}
    G^*F &= \begin{bmatrix}
        G^*_j & G^*_=\\
        G^*_- & G^*_{k-j}
    \end{bmatrix}\begin{bmatrix}E_j & 0_+\\
    F_+ & F_{k-j}\end{bmatrix}\\
    &= \begin{bmatrix}
        G^*_j + G^*_= F_+ & G^*_= F_{k-j}\\
        G^*_- + G^*_{k-j} F_+ & G^*_{k-j}F_{k-j}
    \end{bmatrix}\\
    &= \begin{bmatrix}
        E_j & 0_{j\times (k-j)}\\
        0_{(k-j)\times j} & E_{k-j}
    \end{bmatrix}.
\end{align*}
Thus, $G^*_=F_{k-j}=0_+$, the $j\times (k-j)$ zero matrix. But $\det F = \det(E_j)\cdot \det(F_{k-j})\neq 0$, so $\det F_{k-j}\neq 0$ and we must have $G^*_==0_+$, i.e., $G_= =0_{(k-j)\times j}$. Thus, $G$ is upper-triangular, and so $\det G = \det G_j \cdot \det G_{k-j}\neq 0$, which implies $\det G_{k-j}\neq 0$ and $G_{k-j}$ is nonsingular. Hence,
\[\bar{g} = Gg = \begin{bmatrix}G_j & G_-\\
0_{(k-j)\times j} & G_{k-j}\end{bmatrix} \begin{bmatrix}
g_1\\
\vdots\\
g_k
\end{bmatrix}\]
where $G_{k-j}$ is the nonsingular coefficient matrix such that
\[\begin{bmatrix}
    \bar{g}_{j-1}\\
    \vdots\\
    \bar{g}_{k}
\end{bmatrix} = G_{k-j}\begin{bmatrix}
    g_{j-1}\\
    \vdots\\
    g_{k}
\end{bmatrix}.\]
\end{proof}

We are finally ready to introduce the boundary-form formula, the theorem central to the construction of adjoint boundary condition.

\begin{thm}\cite[p.288]{CoddingtonLevinson}{(Boundary-form formula)}\label{thm:boundary form formula}
    Given any boundary form $U$ of rank $m$ (Definition \ref{defn:boundary form}), and any complementary form $U_c$ (Definition \ref{defn:complementary boundary form}), there exist unique boundary forms $U_c^+$, $U^+$ of rank $m$ and $2n-m$, respectively, such that
    \begin{equation}\label{eq:boundary form formula}
        [xy](b)-[xy](a) = Ux\cdot U_c^+y + U_{c}x\cdot U^+y.
    \end{equation}
    If $\tilde{U}_c$ is any other complementary form to $U$, and $\tilde{U}^+_c, \tilde{U}^+$ the corresponding forms of rank $m$ and $2n-m$, then
    \begin{equation}\label{eq:adjoint boundary forms unique only up to linear transformation}
        \tilde{U}^+ y = C^*U^+y
    \end{equation}
    for some nonsingular matrix $C$.
\end{thm}
\begin{rmk}
    $U^+$ is the key object which will be defined later as an adjoint boundary condition to $U$.

    Note that the existence of $[xy](t)$ implies that a linear differential operator is involved (see (\ref{eq:[uv](t) defn})). The matrices $\hat{B}$ and $B$ in the proof also depend on this linear differential operator.

    Also note that the second statement in the theorem reflects the fact that adjoint boundary conditions are unique only up to linear transformation. This is why when the need for rigor is greater than that for convenience, we take care to use ``an'' instead of ``the'' in referring to adjoint boundary condition.
\end{rmk}
\begin{proof}
    Recall from \eqref{eq:green's formula in semibilinear form} that the left-hand side of \eqref{eq:boundary form formula} can be considered as a semibilinear form $\mathcal{S}(f,g)=\hat{B}f\cdot g$ for vectors
    \[
        f=
        \begin{bmatrix}
            x(a)\\
            \vdots\\
            x^{(n-1)}(a)\\
            x(b)\\
            \vdots\\
            x^{(n-1)}(b)
        \end{bmatrix},\,
        g=
        \begin{bmatrix}
            y(a)\\
            \vdots\\
            y^{(n-1)}(a)\\
            y(b)\\
            \vdots\\
            y^{(n-1)}(b)
        \end{bmatrix}
    \]
    with the nonsingular matrix
    \[
        \hat{B}=
        \begin{bmatrix}
            -B(a) & 0_n\\
            0_n & B(b)
        \end{bmatrix},
    \]
    where $B$ is as in (\ref{eq:B(t)}).
    Recall from a previous discussion that 
    \[Ux = M\xi(a) + N\xi(b) = (M:N)\begin{bmatrix}\xi(a)\\ \xi(b)\end{bmatrix}\]
    for $M, N, \xi$ are as defined there. With the definition of $f$, we have $f=\begin{bmatrix}\xi(a)\\ \xi(b)\end{bmatrix}$ and thus
    \[Ux = (M:N)f.\]
    By Definition \ref{defn:complementary boundary form}, $U_c x = (\tilde{M}:\tilde{N})f$ for two appropriate matrices $\tilde{M}, \tilde{N}$ for which
    \[H = \begin{bmatrix}M & N\\
    \tilde{M} & \tilde{N}\end{bmatrix}_{2n\times 2n}\]
    has rank $2n$. Thus,
    \[\begin{bmatrix}Ux\\ U_cx\end{bmatrix} = \begin{bmatrix}(M:N)f\\(\tilde{M}:\tilde{N})f\end{bmatrix} = \begin{bmatrix}M & N\\
    \tilde{M} & \tilde{N}\end{bmatrix}f = Hf.\]
    By Proposition \ref{prop:unique nonsingular G in semibilinear form}, there exists a unique $2n\times 2n$ nonsingular matrix $J$ (in fact, with $S = \hat{B}$, $F=H$, $J=G$, and $G=(SF^{-1})^\star$ where $^\star$ denotes the conjugate transpose, we have $J=(\hat{B}H^{-1})^\star$) such that $\mathcal{S}(f,g) = Hf\cdot Jg$. Let $U^+, U_c^+$ be such that
    \[Jg = \begin{bmatrix}U_c^+ y\\ U^+y\end{bmatrix},\]
    then 
    \[[xy](b)-[xy](a)=\mathcal{S}(f,g) = Hf\cdot Jg = \begin{bmatrix}Ux\\ U_cx\end{bmatrix}\cdot\begin{bmatrix}U_c^+ y \\ U^+y\end{bmatrix} = Ux\cdot U_c^+y + U_cx\cdot U^+y.\]
    Thus, \eqref{eq:boundary form formula} holds.

    The second statement in the theorem follows from Proposition \ref{prop:last k-j components linear combination} with $Hf$ and $Jg$ corresponding to $f$ and $g$.
\end{proof}

\paragraph{Characterization of the Adjoint}

With the boundary-form formula, we are now able to fully characterize the notion of ``adjoint'' for boundary value problems. In this section, we begin by defining adjoint boundary condition and adjoint boundary value problem. We then explore some properties of these adjoints as relevant to the construction algorithm.

\begin{defn}\cite[p.288-89]{CoddingtonLevinson}\label{defn:adjoint boundary condition}
    For any boundary form $U$ of rank $m$ there is associated the homogeneous boundary condition
    \begin{equation}\label{eq:homogeneous boundary condition}
        Ux=0
    \end{equation}
    for functions $x\in C^{n-1}$ on $[a,b]$. If $U^+$ is any boundary form of rank $2n-m$ determined as in Theorem \ref{thm:boundary form formula}, then the homogeneous boundary condition
    \begin{equation}\label{eq:adjoint boundary condition}
        U^+x=0
    \end{equation}
    is an \textbf{adjoint boundary condition} to (\ref{eq:homogeneous boundary condition}).
\end{defn}

Putting together $L, L^+$ and $U, U^+$, we have the definition of adjoint boundary value problem.

\begin{defn}\cite[p.291]{CoddingtonLevinson}\label{defn:adjoint boundary value problem}
    If $U$ is a boundary form of rank $m$, the problem of finding solutions of
    \[\pi_m:\,Lx=0\quad Ux=0\]
    on $[a,b]$ is a \textbf{homogeneous boundary value problem of rank $m$}. The problem
    \[\pi_{2n-m}^+:\,L^+x=0\quad U^+x=0\]
    on $[a,b]$ is the \textbf{adjoint boundary value problem to $\pi_m$}.
\end{defn}

In connection with the notion of adjoint problem introduced after Definition \ref{defn:homogeneous boundary value problem}, we now have the following property of the adjoint analogous to (\ref{eq:linear map adjoint}).

\begin{prop}\label{prop:(Lu,v)=(u,L^+v)}
    By Green's formula \eqref{eq:green's formula} and the boundary-form formula \eqref{eq:boundary form formula}, 
    \[(Lu, v) = (u, L^+v)\]
    for all $u\in C^n$ on $[a,b]$ satisfying \eqref{eq:homogeneous boundary condition} and all $v\in C^n$ on $[a,b]$ satisfying \eqref{eq:adjoint boundary condition}.
\end{prop}
\begin{proof}
    \begin{equation}
        \begin{split}
            (Lu, v) - (u, L^+v) &= \int_a^b Lu\bar{v}\,dt - \int_a^b u(\overline{L^+v})\,dt\\
            &= [uv](a) - [uv](b) \quad\mbox{(by Green's formula \eqref{eq:green's formula})}\\
            &= Uu\cdot U_c^+ v + U_c u\cdot U^+v \quad\mbox{(by boundary-form formula \eqref{eq:boundary form formula})}\\
            &= 0\cdot U_c^+ v + U_c u \cdot 0 \quad\mbox{(by \eqref{eq:homogeneous boundary condition} and \eqref{eq:adjoint boundary condition})}\\
            &= 0.\qedhere
        \end{split}
    \end{equation}
\end{proof}

Here, we treat the above result as a property of the adjoint after defining $L^+$ and constructing $U^+$. 
%Yet we can still appreciate the motivation behind defining the notion of adjoint for boundary value problems.
In some cases, it is treated as the definition of the adjoint problem, especially if the context wishes to introduce the adjoint problem using linear algebra. Historically, though, it is the adjoint problem that motivated the inner product characterization.

Now, we turn to one last result that would help us in the last step of the construction algorithm, namely checking whether an adjoint boundary condition is valid.

Just like how $U$ is associated with two $m\times n$ matrices $M, N$, $U^+$ is associated with two $n\times (2n-m)$ matrices $P, Q$ such that $(P^*:Q^*)$ has rank $2n-m$ and
\begin{equation}\label{eq:U^+x in P* Q*}
    U^+x = P^*\xi(a) + Q^*\xi(b).
\end{equation}
% Note that embedding $M, N, P^*, Q^*$ in the same matrix gives
% \begin{align*}
%     \begin{bmatrix}
%         (M:N)_{m\times 2n}\\
%         (P^*:Q^*)_{(2n-m)\times 2n}
%     \end{bmatrix}_{2n\times 2n} & = \begin{bmatrix}
%         M & N\\
%         P^* & Q^*
%     \end{bmatrix},
% \end{align*}
% which is a $2n\times 2n$ matrix of full rank.
The following theorem is motivated by characterizing the adjoint condition \eqref{eq:adjoint boundary condition} in terms of the matrices $M, N, P, Q$. For our purpose, it provides a way to check whether the adjoint boundary condition found by the algorithm is indeed valid using the matrices $M, N, P, Q$.

\begin{thm}\cite[p.289]{CoddingtonLevinson}\label{thm:condition iff adjoint}
    The boundary condition $U^+x=0$ is adjoint to $Ux=0$ if and only if
    \begin{equation}\label{eq:condition iff adjoint}
        MB^{-1}(a)P = NB^{-1}(b)Q
    \end{equation}
    where $B(t)$ is the $n\times n$ matrix associated with the form $[xy](t)$ (\ref{eq:B(t)}).
\end{thm}
\begin{proof}
    Let $\eta := (y, y', \ldots, y^{(n-1)})$,
    then $[xy](t)=B(t)\xi(t)\cdot \eta(t)$ by \eqref{[uv](t) in semibilinear form}.

    Suppose $U^+x=0$ is adjoint to $Ux=0$. By definition of adjoint boundary condition (\ref{eq:adjoint boundary condition}), $U^+$ is determined as in Theorem \ref{thm:boundary form formula}. But by Theorem \ref{thm:boundary form formula}, in determining $U^+$, there exist boundary forms $U_c$, $U_c^+$ of rank $2n-m$ and $m$, respectively, such that (\ref{eq:boundary form formula}) holds. 

    Put
    \begin{align*}
        U_cx &= M_c\xi(a) + N_c\xi(b)\quad \rank(M_c:N_c) = 2n-m\\
        U_c^+y &= P_c^*\eta(a) + Q_c^*\eta(b)\quad\rank(P_c^*:Q_c^*)=m.
    \end{align*}
    Then by the boundary-form formula (\ref{eq:boundary form formula}),
    \begin{align*}
        B(b)\xi(b)\cdot \eta(b) - B(a)\xi(a)\cdot \eta(a) &= (M\xi(a) + N\xi(b))\cdot (P_c^*\eta(a) + Q_c^*\eta(b)) + \\
        &\qquad (M_c\xi(a) + N_c\xi(b))\cdot (P^*\eta(a) + Q^*\eta(b)).
    \end{align*}
    By (\ref{eq:semibilinear adjoint}), 
    \[M\xi(a)\cdot P_c^*\eta(a) = P_cM\xi(a)\cdot \eta(a).\]
    Thus,
    \begin{align*}
        B(b)\xi(b)\cdot \eta(b) - B(a)\xi(a)\cdot \eta(a) &= (P_c M + PM_c)\xi(a)\cdot \eta(a) + (Q_cM + QM_c)\xi(a)\cdot \eta(b) \\
        &\qquad (P_cN + PN_c) \xi(b)\cdot \eta(a) + (Q_cN + QN_c) \xi(b)\cdot \eta(b).
    \end{align*}
    Thus, we have
    \begin{align*}
        P_cM + PM_c &= - B(a) & P_cN + PN_c &= 0_n\\
        Q_cM + QM_c &= 0_n & Q_cN + QN_c &= B(b).
    \end{align*}
    Since $\det B(t)\neq 0$ on $t\in[a,b]$, $B^{-1}(a)$, $B^{-1}(b)$ exist, and thus
    \begin{align*}
        \begin{bmatrix}
            -B^{-1}(a)P_c & -B^{-1}(a)P\\
            B^{-1}(b)Q_c & B^{-1}(b)Q
        \end{bmatrix}
        \begin{bmatrix}
            M & N\\
            M_c & N_c
        \end{bmatrix}
        =
        \begin{bmatrix}
            E_n & 0_n\\
            0_n & E_n
        \end{bmatrix}.
    \end{align*}
    Recall that $\begin{bmatrix}
        M & N\\
        M_c & N_c
    \end{bmatrix}$ has full rank, which means that it is nonsingular (Definition \ref{defn:complementary boundary form}). Thus, the two matrices on the left are inverses of each other. So we also have
    \begin{align*}
        \begin{bmatrix}
            M & N\\
            M_c & N_c
        \end{bmatrix}
        \begin{bmatrix}
            -B^{-1}(a)P_c & -B^{-1}(a)P\\
            B^{-1}(b)Q_c & B^{-1}(b)Q
        \end{bmatrix}
        =
        \begin{bmatrix}
            E_m & 0_+\\
            0_- & E_{2n-m}
        \end{bmatrix}.
    \end{align*}
    Therefore,
    \[-MB^{-1}(a)P + NB^{-1}(b)Q = 0_+,\]
    which is \eqref{eq:condition iff adjoint}.

    Conversely, let $U_1^+$ be a boundary form of rank $2n-m$ such that
    \[U_1^+y = P_1^*\eta(a) + Q_1^*\eta(b)\]
    for appropriate $P_1^*$, $Q_1^*$ with $\rank(P_1^*:Q_1^*)=2n-m$. Suppose
    \begin{equation}\label{eq:condition iff adjoint converse}
        MB^{-1}(a)P_1 = NB^{-1}(b)Q_1
    \end{equation}
    holds.

    By the fundamental theorem of linear maps \cite[p.63]{Axler1997}, if $V$ is finite-dimensional and $T\in\mathcal{L}(V, W)$, then $\dim\ker T = \dim V - \dim \range T$. Suppose $A$ is a $n\times k$ matrix, then $A\in\mathcal{L}(\R^n, \R^k)$. Thus, in a homogeneous system of linear equations $Ax=0$, we have $\dim \ker A = \dim A - \dim \range A$. That is, the dimension of solution space $\ker A$ is the difference between the number of unknown variables and the rank of the coefficient matrix, or $\dim\range A$. Therefore, letting $u$ be a $2n\times 1$ vector, there exist exactly $2n-m$ linearly independent solutions of the homogeneous linear system $(M:N)_{m\times 2n}u=0$. By \eqref{eq:condition iff adjoint converse},
    \[MB^{-1}(a)P_1 - NB^{-1}(b)Q=0,\]
    and thus
    \[(M:N)_{m\times 2n}\begin{bmatrix}
        B^{-1}(a)P_1\\
        -B^{-1}(b)Q_1
    \end{bmatrix}_{2n\times (2n-m)} = 0_{m\times (2n-m)}.\]
    So the $2n-m$ columns of the matrix
    \[H_1:= \begin{bmatrix}
        B^{-1}(a)P_1\\
        -B^{-1}(b)Q_1
    \end{bmatrix}\]
    are solutions of this system. Since $\rank(P_1^*:Q_1^*)=2n-m$,
    \[\rank\begin{bmatrix}P_1\\ Q_1\end{bmatrix}=2n-m.\]
    Since $B(a)$, $B(b)$ are nonsingular, $\rank(H_1)=2n-m$.

    If $U^+x=P^*\xi(a) + Q^*\xi(b)=0$ is a boundary condition adjoint to $Ux=0$, then the matrix
    \[\begin{bmatrix}
        -B^{-1}(a)P_c & -B^{-1}(a)P\\
        B^{-1}(b)Q_c & B^{-1}(b)Q
    \end{bmatrix}_{2n\times 2n}\]
is nonsingular (because it has inverse $\begin{bmatrix}M & N\\ M_c & N_c\end{bmatrix}$), i.e., it has full rank. Thus, if 
    \[H = \begin{bmatrix}
        -B^{-1}(a)P\\
        B^{-1}(b)Q
    \end{bmatrix}_{n\times (2n-m)},\]
    then $\rank(H)=2n-m$. Therefore, by \eqref{eq:condition iff adjoint}, the $2n-m$ columns of $H$ also form $2n-m$ linearly independent solutions of $(M:N)u=0$, as in the case of $H_1$. Hence, there exists a nonsingular $(2n-m)\times (2n-m)$ matrix $A$ such that $H_1=HA$ (change of basis in the solution space). 
    Thus we have
    \begin{align*}
        \begin{bmatrix}
            B^{-1}(a)P_1\\
            -B^{-1}(b)Q_1
        \end{bmatrix} = H_1 = HA = \begin{bmatrix}
            B^{-1}(a)PA\\
            -B^{-1}(b)QA
        \end{bmatrix},
    \end{align*}
    or $P_1=PA$, $Q_1=QA$. Thus, 
    \[U_1^+y = P_1^*\eta(a) + Q_1^*\eta(b) = A^*P^*\eta(a) + A^*Q^*\eta(b)= A^* U^+y.\]
    Since $A^\star$ is a linear map, $U^+y=0$ implies $U_1^+y=A^*U^+y=0$. Since $A^\star$ is nonsingular, $A^{\star^{-1}}$ is also a linear map, and $A^{\star^{-1}}U_1^+y = U^+y$. Thus, $U_1^+y=0$ implies $U^+y=A^{\star^{-1}}U_1^+y=0$. Therefore, $U^+y=0$ if and only if $U_1^+y=0$. Since $U^+y=0$ is adjoint to $Ux=0$, $U_1^+y=0$ is adjoint to $Ux=0$.
\end{proof}

To recapitulate, the boundary-form formula (Theorem \ref{thm:boundary form formula}) gives us the existence and construction of adjoint boundary condition, and Theorem \ref{thm:condition iff adjoint} gives us a way to check whether a proposed adjoint boundary condition is valid. Now, we are ready to propose a construction algorithm.

\section{Algorithm}

\subsection{Constructing the Adjoint of a Homogeneous Boundary Condition}
The adjoint boundary condition associated with the adjoint problem relates to that of the original problem in a way that is important to making the Fokas transform pairs work as we would like them to. Thus, the first step in implementing the Fokas method is to construct the adjoint of a homogeneous boundary condition.

To this end, an algorithm to construct adjoint boundary conditions has been devised and implemented in Julia.

\subsection{Constructing the Fokas Transform Pair}
\subsubsection{Approximating the Roots of an Exponential Polynomial on a Bounded Region}

\section{Implementation}
\subsection{Platform}
\subsection{Examples}

\section{Discussion}

\pagebreak
\pagenumbering{gobble}
\addcontentsline{toc}{section}{References}

\newpage
\bibliographystyle{unsrt}
\bibliography{C:/Bibtex/Capstone}
\end{document}

